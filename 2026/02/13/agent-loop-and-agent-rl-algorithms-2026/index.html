

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicon.svg">
  <link rel="icon" href="/images/favicon.svg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#ffffff">
  <meta name="author" content="Nathan">
  <meta name="keywords" content="计算机技术, 生活中的胡思乱想">
  
    <meta name="description" content="引言：从聊天机器人到自主智能体2025-2026 年，AI Agent 迎来了从「对话助手」到「自主执行者」的质变。过去，构建一个 AI Agent 的方法极其简单——拿一个大语言模型（LLM），套一个 while 循环，给它接上工具 API，就能完成简单任务。但当任务变得复杂（比如深度研究、代码重构、多步决策），这种朴素架构就会崩溃。 Agent 领域正在经历一场深刻的范式转移：从基于提示工程的">
<meta property="og:type" content="article">
<meta property="og:title" content="Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景">
<meta property="og:url" content="https://hydraxman.github.io/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/index.html">
<meta property="og:site_name" content="内森淼文">
<meta property="og:description" content="引言：从聊天机器人到自主智能体2025-2026 年，AI Agent 迎来了从「对话助手」到「自主执行者」的质变。过去，构建一个 AI Agent 的方法极其简单——拿一个大语言模型（LLM），套一个 while 循环，给它接上工具 API，就能完成简单任务。但当任务变得复杂（比如深度研究、代码重构、多步决策），这种朴素架构就会崩溃。 Agent 领域正在经历一场深刻的范式转移：从基于提示工程的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hydraxman.github.io/images/agent-loop-react.png">
<meta property="og:image" content="https://hydraxman.github.io/images/cot-vs-tot-comparison.png">
<meta property="og:image" content="https://hydraxman.github.io/images/reflexion-learning-loop.png">
<meta property="og:image" content="https://hydraxman.github.io/images/lats-mcts-agent.png">
<meta property="og:image" content="https://hydraxman.github.io/images/agent-architecture-evolution.png">
<meta property="og:image" content="https://hydraxman.github.io/images/agentic-rl-paradigm.png">
<meta property="article:published_time" content="2026-02-13T13:30:00.000Z">
<meta property="article:modified_time" content="2026-02-20T09:22:12.115Z">
<meta property="article:author" content="Nathan">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="AI Agent">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="深度研究">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://hydraxman.github.io/images/agent-loop-react.png">
  
  
  
  <title>Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景 - 内森淼文</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"hydraxman.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":true,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#2979FF","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 60vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>内森淼文</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/images/banner-dark.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.4)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2026-02-13 21:30" pubdate>
          2026年2月13日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          65 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="AI 前沿"
        id="heading-12740d22eb9407da4ab3c923235c8ddb" role="tab" data-toggle="collapse" href="#collapse-12740d22eb9407da4ab3c923235c8ddb"
        aria-expanded="true"
      >
        AI 前沿
        <span class="list-group-count">(5)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-12740d22eb9407da4ab3c923235c8ddb"
           role="tabpanel" aria-labelledby="heading-12740d22eb9407da4ab3c923235c8ddb">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/2026/02/20/ai-models-feb-2026/" title="Claude、GPT、Gemini 二月大乱斗：到底谁才是真正的王者？"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Claude、GPT、Gemini 二月大乱斗：到底谁才是真正的王者？</span>
        </a>
      
    
      
      
        <a href="/2026/02/19/what-is-agent-skills/" title="AI Agent 的「技能树」：Agent Skills 运行原理、路由机制与 SDK 集成全解析"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AI Agent 的「技能树」：Agent Skills 运行原理、路由机制与 SDK 集成全解析</span>
        </a>
      
    
      
      
        <a href="/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/" title="Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</span>
        </a>
      
    
      
      
        <a href="/2026/02/10/ai-agent-era-2026/" title="2026年，AI Agent 时代真的来了"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">2026年，AI Agent 时代真的来了</span>
        </a>
      
    
      
      
        <a href="/2026/02/06/claude-opus-4-6-vs-gpt-5-3-codex/" title="Claude Opus 4.6 vs GPT-5.3-Codex：2026年AI编程双雄同日对决"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Claude Opus 4.6 vs GPT-5.3-Codex：2026年AI编程双雄同日对决</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="引言：从聊天机器人到自主智能体"><a href="#引言：从聊天机器人到自主智能体" class="headerlink" title="引言：从聊天机器人到自主智能体"></a>引言：从聊天机器人到自主智能体</h2><p>2025-2026 年，AI Agent 迎来了从「对话助手」到「自主执行者」的质变。过去，构建一个 AI Agent 的方法极其简单——拿一个大语言模型（LLM），套一个 while 循环，给它接上工具 API，就能完成简单任务。但当任务变得复杂（比如深度研究、代码重构、多步决策），这种朴素架构就会崩溃。</p>
<p>Agent 领域正在经历一场深刻的范式转移：<strong>从基于提示工程的静态 Agent，走向基于强化学习的自适应 Agent</strong>。本文将系统梳理驱动 Agent 完成长任务的各类算法与架构，从经典的 Agent Loop 到前沿的 Agentic RL。</p>
<hr>
<h2 id="一、Agent-Loop：基础循环架构"><a href="#一、Agent-Loop：基础循环架构" class="headerlink" title="一、Agent Loop：基础循环架构"></a>一、Agent Loop：基础循环架构</h2><h3 id="1-1-最简-Agent-循环"><a href="#1-1-最简-Agent-循环" class="headerlink" title="1.1 最简 Agent 循环"></a>1.1 最简 Agent 循环</h3><p>最基础的 Agent 架构可以抽象为一个循环：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css">while not done:<br>    thought = LLM.<span class="hljs-built_in">think</span>(context)<br>    action = LLM.<span class="hljs-built_in">decide</span>(thought)<br>    observation = environment.<span class="hljs-built_in">execute</span>(action)<br>    context.<span class="hljs-built_in">append</span>(observation)<br></code></pre></td></tr></table></figure>

<p>这就是所谓的 <strong>Agent 1.0 架构</strong>。LLM 充当「大脑」，在循环中反复执行「思考→行动→观察」直到任务完成或达到终止条件。</p>
<h3 id="1-2-ReAct：推理与行动的交错"><a href="#1-2-ReAct：推理与行动的交错" class="headerlink" title="1.2 ReAct：推理与行动的交错"></a>1.2 ReAct：推理与行动的交错</h3><p><strong>ReAct（Reasoning and Acting）</strong> 是最经典的 Agent Loop 框架，由 Yao 等人于 2022 年提出。其核心思想是让 LLM 交替生成推理步骤和执行动作：</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs fortran">循环流程：<br>Thought → <span class="hljs-keyword">Action</span> → Observation → Thought → <span class="hljs-keyword">Action</span> → Observation → ... → <span class="hljs-keyword">Final</span> Answer<br></code></pre></td></tr></table></figure>

<p><strong>关键设计：</strong></p>
<ul>
<li><strong>Thought（思考）</strong>：LLM 内部推理，分析当前状态，制定下一步计划</li>
<li><strong>Action（行动）</strong>：调用外部工具（搜索、计算、API 等）</li>
<li><strong>Observation（观察）</strong>：接收工具返回结果，更新上下文</li>
</ul>
<p>ReAct 的优势在于推理过程透明可追踪，但在长任务中会遇到严重问题：上下文窗口被大量历史信息污染，导致模型「迷失方向」。</p>
<p><img src="/images/agent-loop-react.png" srcset="/img/loading.gif" lazyload alt="Agent Loop ReAct 架构"><br><em>图：ReAct 的核心循环——Thought（推理）→ Action（行动）→ Observation（观察）交替执行，直到任务完成。</em></p>
<h3 id="1-3-Plan-and-Execute：先规划后执行"><a href="#1-3-Plan-and-Execute：先规划后执行" class="headerlink" title="1.3 Plan-and-Execute：先规划后执行"></a>1.3 Plan-and-Execute：先规划后执行</h3><p>为解决 ReAct 在长任务中的漂移问题，<strong>Plan-and-Execute</strong> 架构将任务分为两个阶段：</p>
<ol>
<li><strong>规划阶段（Planner）</strong>：LLM 分析任务，生成分步计划</li>
<li><strong>执行阶段（Executor）</strong>：按计划逐步执行，每步可调用工具</li>
<li><strong>重规划（Re-Planner）</strong>：根据执行结果动态调整剩余计划</li>
</ol>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs smali">Plan: [Step1, Step2, Step3, Step4]<br>Execute Step1 → Result1<br>Re-Plan: [Step2&#x27;, Step3, Step4]  // 根据 Result1 调整<br>Execute Step2&#x27; → Result2<br><span class="hljs-keyword">.</span>..<br></code></pre></td></tr></table></figure>

<p>这种架构的优势是目标感更强，不容易在长链推理中丧失方向。</p>
<h3 id="1-4-ReWOO：推理与观察解耦"><a href="#1-4-ReWOO：推理与观察解耦" class="headerlink" title="1.4 ReWOO：推理与观察解耦"></a>1.4 ReWOO：推理与观察解耦</h3><p><strong>ReWOO（Reasoning Without Observation）</strong> 进一步优化了 Plan-and-Execute 模式：</p>
<ul>
<li>先一次性生成完整的推理计划和所有工具调用</li>
<li>并行执行所有工具调用</li>
<li>最后综合所有结果生成答案</li>
</ul>
<p>优势是减少 LLM 调用次数，提升效率；但牺牲了动态调整能力。</p>
<hr>
<h2 id="二、高级推理框架：从线性到树状再到自我进化"><a href="#二、高级推理框架：从线性到树状再到自我进化" class="headerlink" title="二、高级推理框架：从线性到树状再到自我进化"></a>二、高级推理框架：从线性到树状再到自我进化</h2><p>上一节的 Agent Loop 架构解决了”如何让 LLM 与外部世界交互”的问题，但它们都面临一个共同瓶颈：<strong>推理质量</strong>。ReAct 按顺序执行每一步，一旦某步方向错误，整个链条就会偏离正确路径。Plan-and-Execute 有规划能力，但规划本身也可能出错，且无法在推理层面进行深度探索。</p>
<p>这就引出了一个核心问题：<strong>如何让 LLM 在推理过程中更智能地搜索和探索？</strong></p>
<p>答案隐藏在三个递进的范式中：CoT（线性思考）→ ToT（树状探索）→ Reflexion&#x2F;LATS（自我进化的搜索）。</p>
<h3 id="2-1-Chain-of-Thought（CoT）：思维链推理"><a href="#2-1-Chain-of-Thought（CoT）：思维链推理" class="headerlink" title="2.1 Chain-of-Thought（CoT）：思维链推理"></a>2.1 Chain-of-Thought（CoT）：思维链推理</h3><p>CoT 是 Agent 推理的基石。2022 年 Wei 等人在 Google Brain 的工作揭示了一个关键发现：只需在提示中加入”Let’s think step by step”，就能让 LLM 将复杂问题拆解为一系列中间推理步骤，显著提升数学、逻辑和常识推理的准确率。</p>
<p><strong>CoT 的核心机制：</strong></p>
<ul>
<li>LLM 不再直接输出最终答案，而是生成一条<strong>线性推理链</strong></li>
<li>每个中间步骤为后续步骤提供额外的”证据”或约束条件</li>
<li>从概率角度看，这相当于对模型输出分布的贝叶斯更新——每一步都缩小了解空间</li>
</ul>
<p><strong>一个典型例子：</strong></p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs tap">问题：停车场有3排车，每排8辆，又开走了4辆，还剩多少辆？<br><br>CoT 推理链：<br>Step 1: 总共有<span class="hljs-number"> 3 </span>×<span class="hljs-number"> 8 </span>=<span class="hljs-number"> 24 </span>辆车<br>Step 2: 开走了<span class="hljs-number"> 4 </span>辆<br>Step 3: 剩下<span class="hljs-number"> 24 </span>-<span class="hljs-number"> 4 </span>=<span class="hljs-number"> 20 </span>辆<br>答案：20 辆<br></code></pre></td></tr></table></figure>

<p>然而，CoT 作为一条<strong>单一的线性链条</strong>，存在三个根本性局限：</p>
<p><strong>① 不可回溯（No Backtracking）</strong><br>一旦某一步推理出错，错误会沿着链条一路传播，无法返回修正。就像在迷宫中只能一直往前走，走错了也不能回头。</p>
<p><strong>② 无法探索多条路径（No Branching）</strong><br>面对有多种可能解法的问题，CoT 只能选择一条路走到底。比如解数学题时，可能有代数法和几何法两条路径，CoT 只会选择一条。</p>
<p><strong>③ 缺乏自我评估（No Self-Evaluation）</strong><br>链条上的每一步都没有被评估是否合理，模型无法判断当前方向是否正确，只能盲目前进。</p>
<p>CoT-SC（Self-Consistency）通过<strong>并行采样多条独立链条</strong>然后投票选择最佳答案，部分缓解了上述问题——但每条链条之间仍然是完全独立的，无法共享中间发现，也无法在关键决策点分叉探索。</p>
<h3 id="2-2-从-CoT-到-ToT：为什么需要进化？"><a href="#2-2-从-CoT-到-ToT：为什么需要进化？" class="headerlink" title="2.2 从 CoT 到 ToT：为什么需要进化？"></a>2.2 从 CoT 到 ToT：为什么需要进化？</h3><p>正是 CoT 的上述三个局限催生了 Tree-of-Thoughts（ToT）。让我们通过一个经典问题来理解这个进化的必要性：</p>
<p><strong>Game of 24 问题</strong>：用 4、5、6、10 四个数字和加减乘除，组合出结果为 24。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">CoT 的困境：<br><span class="hljs-keyword">Step</span> 1: 尝试 4 × 6 = 24<span class="hljs-built_in">..</span>. 但还剩 5 和 10 没用<br><span class="hljs-keyword">Step</span> 2: 好像不行，但已经无法回头了<br>→ 失败，只能从头再来一次（但新的一次完全独立，不会记住上次的教训）<br><br>ToT 的优势：<br>          4, 5, 6, 10<br>        /      |       \<br>   4+<span class="hljs-attribute">5</span>=9    4×<span class="hljs-attribute">6</span>=24    <span class="hljs-attribute">10-6</span>=4<br>   /    \     ✗(剩余无法凑)  /   \<br> 9×(10-6)  6×(10-5)   4×<span class="hljs-attribute">5</span>=20  <span class="hljs-built_in">..</span>.<br> =9×<span class="hljs-attribute">4</span>=36✗  =6×<span class="hljs-attribute">5</span>=30✗   20+<span class="hljs-attribute">4</span>=24? ✗(4已用)<br>                        ↑ 回溯，换路<br></code></pre></td></tr></table></figure>

<p>ToT 在每一步都可以<strong>分叉探索多个方向</strong>，在发现死路时<strong>回溯</strong>到之前的节点尝试其他路径。</p>
<h3 id="2-3-Tree-of-Thoughts（ToT）：树状搜索推理"><a href="#2-3-Tree-of-Thoughts（ToT）：树状搜索推理" class="headerlink" title="2.3 Tree-of-Thoughts（ToT）：树状搜索推理"></a>2.3 Tree-of-Thoughts（ToT）：树状搜索推理</h3><p>ToT（由 Yao 等人于 2023 年在普林斯顿提出）将推理从线性链扩展为<strong>树状结构</strong>，本质上是将经典搜索算法引入 LLM 推理过程。</p>
<p><strong>ToT 的四大核心组件：</strong></p>
<p><strong>① 思维分解（Thought Decomposition）</strong><br>将问题拆解为适当粒度的”思维单元”。粒度选择至关重要——太细则搜索空间爆炸，太粗则失去探索灵活性。比如写一篇文章，一个思维单元可以是”段落大纲”而非”单个句子”。</p>
<p><strong>② 思维生成（Thought Generation）</strong><br>在每个节点生成 k 个候选思维，有两种策略：</p>
<ul>
<li><strong>采样（Sample）</strong>：独立生成多个候选（适合创意性任务，解空间大）</li>
<li><strong>提议（Propose）</strong>：基于前文依次生成（适合逻辑性任务，避免重复）</li>
</ul>
<p><strong>③ 状态评估（State Evaluation）</strong><br>这是 ToT 最关键的创新——<strong>用 LLM 自己来评估每个中间状态的质量</strong>：</p>
<ul>
<li><strong>打分法</strong>：对每个状态评分（如 1-10 分，或”确定&#x2F;可能&#x2F;不可能”）</li>
<li><strong>投票法</strong>：让 LLM 比较多个候选，选出最有前途的</li>
</ul>
<p>评估函数充当了”导航仪”的角色，告诉搜索算法哪些方向值得继续探索、哪些应该剪枝放弃。</p>
<p><strong>④ 搜索算法（Search Algorithm）</strong><br>ToT 支持两种经典搜索策略：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">BFS</span>（广度优先）：                    <span class="hljs-selector-tag">DFS</span>（深度优先）：<br>层层扩展，不遗漏                     一条路走到底，走不通再回头<br><br><span class="hljs-selector-tag">Level</span> <span class="hljs-number">0</span>:    <span class="hljs-selector-attr">[A]</span>                     探索顺序：<span class="hljs-selector-tag">A</span> → <span class="hljs-selector-tag">B</span> → <span class="hljs-selector-tag">D</span> → (回溯) → <span class="hljs-selector-tag">E</span> → (回溯)<br><span class="hljs-selector-tag">Level</span> <span class="hljs-number">1</span>:  <span class="hljs-selector-attr">[B]</span> <span class="hljs-selector-attr">[C]</span>                              → <span class="hljs-selector-tag">C</span> → <span class="hljs-selector-tag">F</span> → ✓ 找到解<br><span class="hljs-selector-tag">Level</span> <span class="hljs-number">2</span>: <span class="hljs-selector-attr">[D]</span><span class="hljs-selector-attr">[E]</span><span class="hljs-selector-attr">[F]</span><span class="hljs-selector-attr">[G]</span><br><br>适合：解空间较小，需要最优解           适合：解空间大，需要尽快找到一个可行解<br></code></pre></td></tr></table></figure>

<p><strong>ToT 解决了 CoT 的三大痛点：</strong></p>
<table>
<thead>
<tr>
<th>CoT 的局限</th>
<th>ToT 的解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>不可回溯</td>
<td>DFS 自然支持回溯，发现死路可返回上一节点</td>
</tr>
<tr>
<td>无法分叉探索</td>
<td>每个节点可生成 k 个候选分支</td>
</tr>
<tr>
<td>缺乏自我评估</td>
<td>状态评估函数在每步进行质量判断</td>
</tr>
</tbody></table>
<p><strong>代价是什么？</strong> ToT 需要更多的 LLM 调用（生成 + 评估），计算成本显著高于 CoT。这是”搜索质量”与”计算成本”之间的经典权衡——和 AlphaGo 的蒙特卡洛树搜索是同一种思想。</p>
<p><img src="/images/cot-vs-tot-comparison.png" srcset="/img/loading.gif" lazyload alt="CoT vs ToT 推理结构对比"><br><em>图：CoT 线性链 vs ToT 树状搜索的结构差异。CoT 只能沿单一路径前进，ToT 在每步分叉并评估，支持回溯探索。</em></p>
<h3 id="2-4-Reflexion：自我反思学习"><a href="#2-4-Reflexion：自我反思学习" class="headerlink" title="2.4 Reflexion：自我反思学习"></a>2.4 Reflexion：自我反思学习</h3><p>ToT 解决了”单次推理中的探索问题”，但还有一个更深层的问题没有解决：<strong>跨任务的经验积累</strong>。人类在失败后会反思总结教训，下次遇到类似问题时表现更好。CoT 和 ToT 都没有这种”从失败中学习”的能力——每次推理都从零开始。</p>
<p><strong>Reflexion</strong> 引入了一个关键的「反思」闭环，让 Agent 能在多次尝试之间积累经验：</p>
<figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">Trial 1</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Actor 执行 → 得到结果 → Evaluator 评判 → 失败 ❌</span><br>         <span class="hljs-attribute">↓</span><br><span class="hljs-attribute">    Self-Reflection</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我在第3步选错了API，应该用search而不是lookup&quot;</span><br>         <span class="hljs-attribute">↓  反思摘要存入长期记忆</span><br><span class="hljs-attribute">Trial 2</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Actor 带着反思记忆重新执行 → 改进但仍有问题 ❌</span><br>         <span class="hljs-attribute">↓</span><br><span class="hljs-attribute">    Self-Reflection</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;API调对了，但参数格式不对，应该用JSON而非字符串&quot;</span><br>         <span class="hljs-attribute">↓  追加到长期记忆</span><br><span class="hljs-attribute">Trial 3</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Actor 综合两次教训执行 → 成功 ✅</span><br></code></pre></td></tr></table></figure>

<p>Reflexion 包含三个核心组件的协作循环：</p>
<ul>
<li><strong>Actor（执行器）</strong>：基于 ReAct 或 CoT 的执行引擎，负责实际操作</li>
<li><strong>Evaluator（评估器）</strong>：判断执行结果的成功&#x2F;失败，提供二元或标量反馈信号</li>
<li><strong>Self-Reflection（反思器）</strong>：最核心的创新——将失败经验转化为自然语言反思摘要，存入一个持久的<strong>语言记忆（verbal memory）</strong></li>
</ul>
<p><strong>为什么用语言记忆而不是梯度更新？</strong> 这是 Reflexion 最巧妙的设计——传统 RL 通过更新模型权重来学习，成本极高且需要大量样本。Reflexion 用自然语言存储教训（如”不要用 deprecated API v1，改用 v2”），轻量、可解释，而且在推理时通过上下文注入即可使用。</p>
<p><strong>Reflexion 在编程任务上的惊人效果：</strong></p>
<ul>
<li>HumanEval 上从 80.1%（CoT 基线）提升到 91.0%（+11%）</li>
<li>其中约 40% 的错误在第二次尝试时就被修正</li>
<li>这证明了”反思+重试”机制的强大——很多错误不需要更强的模型，只需要从失败中学到教训</li>
</ul>
<p><img src="/images/reflexion-learning-loop.png" srcset="/img/loading.gif" lazyload alt="Reflexion 自我反思学习循环"><br><em>图：Reflexion 的三组件反思循环——Actor 执行、Evaluator 评判、Self-Reflection 生成语言化教训存入记忆，驱动下一次尝试改进。</em></p>
<h3 id="2-5-从-ToT-到-LATS：统一搜索、行动与反思"><a href="#2-5-从-ToT-到-LATS：统一搜索、行动与反思" class="headerlink" title="2.5 从 ToT 到 LATS：统一搜索、行动与反思"></a>2.5 从 ToT 到 LATS：统一搜索、行动与反思</h3><p>到这里，我们已经有了三种关键能力：</p>
<ul>
<li><strong>CoT&#x2F;ToT</strong>：推理时的搜索与探索</li>
<li><strong>ReAct</strong>：与外部环境的交互（工具调用）</li>
<li><strong>Reflexion</strong>：从失败中学习</li>
</ul>
<p>但这三种能力是<strong>各自独立</strong>的。ToT 只做推理搜索，不调用工具；ReAct 调用工具但不做搜索；Reflexion 做反思但搜索策略很原始。有没有一个框架能把这三者统一起来？</p>
<p><strong>LATS（Language Agent Tree Search）</strong> 正是这个统一框架。它将蒙特卡洛树搜索（MCTS）——AlphaGo 的核心算法——引入 LLM Agent 决策，将搜索、行动和反思融合为一个整体：</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs scss">                   ┌──────────────────────┐<br>                   │    MCTS 搜索循环      │<br>                   └──────────────────────┘<br>                             │<br>   ┌─────────────┬──────────┼──────────┬──────────────┐<br>   ▼             ▼          ▼          ▼              ▼<br>Selection    Expansion  Simulation  Evaluation   Backpropagation<br>(选择节点)   (生成动作)  (执行动作)  (LLM评分)    (反向传播)<br>   │             │          │          │              │<br> UCB1算法    LLM生成     调用工具    价值评估      更新节点<br> 平衡探索     候选动作    获取反馈    + 反思        质量分数<br> 与利用                              <br></code></pre></td></tr></table></figure>

<p><strong>MCTS 五步循环详解：</strong></p>
<p><strong>① Selection（选择）</strong>：用 UCB1 公式选择最值得探索的节点，自动平衡”深入已知好路径”和”尝试未探索方向”——这解决了 ToT 简单 BFS&#x2F;DFS 策略的效率问题。</p>
<p><strong>② Expansion（扩展）</strong>：在选中节点用 LLM 生成 n 个候选动作，每个动作可以是推理步骤或工具调用——这融合了 ToT 的分支能力和 ReAct 的工具交互。</p>
<p><strong>③ Simulation（模拟）</strong>：执行动作并观察环境反馈——ReAct 的核心循环。</p>
<p><strong>④ Evaluation（评估）</strong>：LLM 对当前状态进行价值评估，给出分数。<strong>关键创新：如果检测到失败，触发 Reflexion 式的自我反思，生成反思摘要指导后续搜索。</strong></p>
<p><strong>⑤ Backpropagation（反向传播）</strong>：将评估分数沿树路径回传，更新每个祖先节点的质量估计——这让 LATS 能从全局视角优化搜索方向。</p>
<p><strong>LATS &#x3D; ToT（搜索框架）+ ReAct（环境交互）+ Reflexion（失败学习）</strong></p>
<p>这种统一带来的效果是显著的：在 HotPotQA 多跳推理任务上，LATS 比单独的 ReAct 提升 16%，比 Reflexion 提升 8%，比 ToT 提升 12%。代价是更高的计算成本——但这正是”用推理时计算换取更好决策”的核心思想。</p>
<p><img src="/images/lats-mcts-agent.png" srcset="/img/loading.gif" lazyload alt="LATS 蒙特卡洛树搜索 Agent 决策"><br><em>图：LATS 将 MCTS 的五步循环应用于 Agent 决策，统一了搜索（ToT）、行动（ReAct）和反思（Reflexion）三大能力。</em></p>
<hr>
<h2 id="三、Deep-Agent-架构：当任务跨越百步"><a href="#三、Deep-Agent-架构：当任务跨越百步" class="headerlink" title="三、Deep Agent 架构：当任务跨越百步"></a>三、Deep Agent 架构：当任务跨越百步</h2><p>LATS 统一了搜索、行动和反思，但它仍然在<strong>单个上下文窗口</strong>内运作。当任务复杂度从”几步完成”升级到”数十甚至上百步”——比如写一份完整的研究报告、重构一个大型代码库、或执行一个跨越数小时的深度研究——上下文窗口就成了不可逾越的瓶颈：推理历史、工具返回、中间结果……全部挤在有限的 token 窗口中，信噪比急剧下降。</p>
<p><strong>Deep Agent（深度智能体）</strong> 架构是 2025 年下半年兴起的新范式，代表产品包括 Claude Code、OpenAI Deep Research、Manus AI 等。它的核心思想是：**将 Agent 的认知从”上下文内”扩展到”上下文外”**——用外部持久化系统弥补上下文窗口的局限。</p>
<p><img src="/images/agent-architecture-evolution.png" srcset="/img/loading.gif" lazyload alt="Agent 架构演进全景图"><br><em>图：从 ReAct 到 Deep Agent 的架构演进。每一步进化都在解决前一代的核心瓶颈：ReAct 解决了工具交互、CoT&#x2F;ToT 解决了推理搜索、Reflexion 解决了经验学习、Deep Agent 解决了长任务上下文管理。</em></p>
<h3 id="3-1-四大支柱"><a href="#3-1-四大支柱" class="headerlink" title="3.1 四大支柱"></a>3.1 四大支柱</h3><p>Deep Agent 架构建立在四个基础之上：</p>
<p><strong>① 显式规划（Explicit Planning）</strong><br>不依赖 LLM 隐式推理，而是维护一个<strong>外部的、可持久化的任务计划</strong>。计划可以被检查、修改和恢复。这意味着即使上下文窗口被清空，Agent 仍然知道自己在做什么、做到了哪一步。</p>
<p>以 Claude Code 为例：当它重构一个大型代码库时，会在文件系统中写入一份 <code>plan.md</code>，记录每个模块的改造状态。即使中间因为上下文溢出导致会话重置，Agent 读取 plan.md 后就能无缝继续。</p>
<p><strong>② 层级委派（Hierarchical Delegation）</strong><br>单个 Agent 的能力总有上限。Deep Agent 将复杂任务拆分给<strong>专门化的子 Agent</strong>，每个子 Agent 有独立的上下文窗口和专属工具集：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">Orchestrator </span>Agent（指挥官）<br>├── Research <span class="hljs-keyword">Sub-Agent（负责信息检索）→ </span>有搜索工具<br>├── Code <span class="hljs-keyword">Sub-Agent（负责代码编写）→ </span>有文件读写和终端<br>├── Review <span class="hljs-keyword">Sub-Agent（负责质量审查）→ </span>有测试工具<br>└── Memory <span class="hljs-keyword">Sub-Agent（负责信息管理）→ </span>有知识库<br></code></pre></td></tr></table></figure>

<p>这种设计的关键优势是<strong>上下文隔离</strong>：Research Agent 的搜索结果不会污染 Code Agent 的编码上下文。每个子 Agent 只接收与自己任务相关的信息，信噪比大幅提升。</p>
<p><strong>③ 持久化记忆（Persistent Memory）</strong><br>使用文件系统作为外部记忆，而非仅依赖上下文窗口。Agent 可以：</p>
<ul>
<li>写入结构化笔记和发现摘要</li>
<li>读取之前的研究结果</li>
<li>维护状态文件跟踪进度</li>
<li>建立知识索引便于快速检索</li>
</ul>
<p>这本质上是将人类研究员的”做笔记”习惯编码为 Agent 的核心行为——上下文窗口是”工作记忆”（短期），文件系统是”笔记本”（长期）。</p>
<p><strong>④ 极致的上下文工程（Extreme Context Engineering）</strong><br>精心管理什么信息进入上下文窗口。具体技术包括：</p>
<ul>
<li><strong>渐进式摘要</strong>：每隔 N 步将历史压缩为摘要</li>
<li><strong>选择性加载</strong>：只加载与当前子任务相关的信息</li>
<li><strong>上下文分层</strong>：系统提示 &gt; 当前任务 &gt; 相关历史 &gt; 可选参考</li>
<li><strong>智能截断</strong>：工具返回过长时自动截取关键部分</li>
</ul>
<p>以 OpenClaw 为例——它在每次 heartbeat 时读取 HEARTBEAT.md（而非全部历史），在每个 session 开始时读取 SOUL.md 和 USER.md（身份信息），只有在主 session 中才加载 MEMORY.md（长期记忆），这就是上下文工程的实际应用。</p>
<h3 id="3-2-CORAL：认知资源自分配"><a href="#3-2-CORAL：认知资源自分配" class="headerlink" title="3.2 CORAL：认知资源自分配"></a>3.2 CORAL：认知资源自分配</h3><p><strong>CORAL（Cognitive Resource Self-Allocation）</strong> 是 ICLR 2026 收录的工作，专门解决长任务中 Agent 的「注意力漂移」问题——当上下文中积累了太多无关信息，LLM 的注意力被分散，推理质量急剧下降。</p>
<p><strong>核心洞察：</strong> 人类处理长任务时，会主动”清空短期记忆”——比如写论文写了3小时后，会先休息，回来后重新读一遍大纲，而不是试图记住之前的每一个细节。CORAL 给 Agent 提供了类似的能力。</p>
<p><strong>工作记忆管理工具集：</strong></p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs scss">Agent 工具箱中新增三种&quot;元工具&quot;：<br><span class="hljs-number">1</span>. <span class="hljs-built_in">set_checkpoint</span>(label)  → 在关键节点保存状态快照<br><span class="hljs-number">2</span>. <span class="hljs-built_in">clear_memory</span>()         → 清除工作记忆中的杂乱信息  <br><span class="hljs-number">3</span>. <span class="hljs-built_in">restore</span>(label)         → 从指定检查点恢复推理上下文<br></code></pre></td></tr></table></figure>

<p>当 Agent 探索了多条路径、积累了大量搜索结果，发现自己”迷失方向”时，可以主动调用 <code>clear_memory()</code> + <code>restore(&quot;initial_plan&quot;)</code> 来重新开始——但不是完全从零开始，而是保留了检查点中的关键发现。</p>
<p><strong>训练方式：</strong> CORAL 使用<strong>多轮 Agentic 强化策略优化（Multi-episode Agentic Reinforced Policy Optimization）</strong> 算法，让 Agent 通过 RL 学会三个关键判断：</p>
<ul>
<li><strong>何时设置检查点</strong>（在做出重要发现或关键决策后）</li>
<li><strong>何时清理记忆</strong>（当上下文信噪比过低时）</li>
<li><strong>恢复到哪个检查点</strong>（选择最有价值的历史状态）</li>
</ul>
<p>CORAL 在 SWE-bench 等长任务基准上显著优于没有记忆管理的 Agent，验证了”主动管理认知资源”的价值——这也为 Deep Agent 架构的”极致上下文工程”提供了理论支撑。</p>
<hr>
<h2 id="四、Agentic-RL：从工程拼接到端到端学习"><a href="#四、Agentic-RL：从工程拼接到端到端学习" class="headerlink" title="四、Agentic RL：从工程拼接到端到端学习"></a>四、Agentic RL：从工程拼接到端到端学习</h2><p>前三节的所有架构——从 ReAct 到 LATS 到 Deep Agent——都属于「推理时（inference-time）」的工程技巧。它们通过精巧的提示设计、搜索算法和记忆管理来提升 Agent 表现，但<strong>模型本身并没有因此变得更强</strong>。模型权重是冻结的，所有的”聪明”都来自外部框架。</p>
<p>这就像给一个普通人配备了最好的工具箱、最详细的操作手册——他确实能完成更复杂的任务，但他自身的能力并没有提升。如果工具箱被拿走或手册不适用，他就回到了原点。</p>
<p><strong>Agentic RL</strong> 代表了一个根本性的范式转移：<strong>直接通过强化学习训练 LLM 的 Agent 行为能力</strong>，让模型在多步交互中学会规划、工具使用、错误修正——这些能力被编码进模型权重，而非依赖外部框架。</p>
<h3 id="4-1-从-RLHF-到-Agentic-RL"><a href="#4-1-从-RLHF-到-Agentic-RL" class="headerlink" title="4.1 从 RLHF 到 Agentic RL"></a>4.1 从 RLHF 到 Agentic RL</h3><table>
<thead>
<tr>
<th>维度</th>
<th>RLHF</th>
<th>Agentic RL</th>
</tr>
</thead>
<tbody><tr>
<td>目标</td>
<td>让 LLM 输出更符合人类偏好</td>
<td>让 LLM 学会多步决策与工具使用</td>
</tr>
<tr>
<td>交互</td>
<td>单轮：prompt → response</td>
<td>多轮：action → env feedback → action</td>
</tr>
<tr>
<td>奖励</td>
<td>人类偏好评分</td>
<td>任务完成度 + 过程奖励</td>
</tr>
<tr>
<td>训练格式</td>
<td>单条序列</td>
<td>多轮轨迹（trajectory）</td>
</tr>
<tr>
<td>环境</td>
<td>无</td>
<td>真实或模拟环境</td>
</tr>
</tbody></table>
<p>Agentic RL 的核心突破在于：<strong>将 LLM 从被动的序列生成器重新定义为主动的、嵌入复杂动态世界的决策智能体。</strong></p>
<p><img src="/images/agentic-rl-paradigm.png" srcset="/img/loading.gif" lazyload alt="Agentic RL 范式转移"><br><em>图：从推理时工程（左）到 Agentic RL 端到端训练（右）的范式转移——外部脚手架 vs 内化能力。</em></p>
<h3 id="4-2-Agent-R1：端到端-Agent-强化学习"><a href="#4-2-Agent-R1：端到端-Agent-强化学习" class="headerlink" title="4.2 Agent-R1：端到端 Agent 强化学习"></a>4.2 Agent-R1：端到端 Agent 强化学习</h3><p><strong>Agent-R1</strong>（中国科学技术大学，2025.11）是将 DeepSeek-R1 的 RL 训练范式扩展到 Agent 场景的里程碑工作。</p>
<p><strong>核心问题：为什么不能直接把 RLHF&#x2F;GRPO 套到 Agent 上？</strong></p>
<p>传统 RL 训练 LLM 时，轨迹（trajectory）是一个单轮序列：prompt → response。但 Agent 的轨迹是多轮交互序列，包含两种本质不同的 token：</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sqf">轨迹结构：<br>[System Prompt] → <span class="hljs-built_in">Agent</span>生成思考 → <span class="hljs-built_in">Agent</span>调用工具 → 环境返回结果 → <span class="hljs-built_in">Agent</span>继续思考 → ...<br>                  ↑ <span class="hljs-built_in">Agent</span> token（可训练）      ↑ 环境 token（不可训练！）<br></code></pre></td></tr></table></figure>

<p>关键区分：<strong>Agent 生成的 token 需要参与梯度计算，但环境返回的 token 不应该</strong>——因为你不能通过训练模型来改变环境的行为。Agent-R1 在 MDP 框架中明确建模了这个区分。</p>
<p><strong>MDP 扩展详解：</strong></p>
<table>
<thead>
<tr>
<th>组件</th>
<th>静态 LLM</th>
<th>Agent-R1</th>
</tr>
</thead>
<tbody><tr>
<td><strong>状态空间</strong></td>
<td>prompt + 已生成 token</td>
<td>完整对话历史 + 每轮环境反馈</td>
</tr>
<tr>
<td><strong>动作空间</strong></td>
<td>词表中选下一个 token</td>
<td>同上，但 token 序列可触发工具调用</td>
</tr>
<tr>
<td><strong>转移函数</strong></td>
<td>确定性（拼接 token）</td>
<td><strong>随机性</strong>（环境返回不确定）</td>
</tr>
<tr>
<td><strong>奖励函数</strong></td>
<td>单次终端奖励</td>
<td>终端奖励 + 中间过程奖励</td>
</tr>
</tbody></table>
<p><strong>过程奖励（Process Rewards）</strong> 是 Agent-R1 的重要创新——不只在任务完成时给奖励，在中间步骤也给信号。比如：正确调用了 search API 但查询词不够精确，可以给一个小的正奖励（鼓励工具使用）但不是满分（查询还需优化）。这解决了长任务中”奖励稀疏”的经典难题。</p>
<p>Agent-R1 开源了完整的训练框架（基于 veRL），支持快速接入不同环境，已在 Multi-hop QA 上验证了效果。</p>
<h3 id="4-3-AgentRL：多任务多轮-Agent-训练框架"><a href="#4-3-AgentRL：多任务多轮-Agent-训练框架" class="headerlink" title="4.3 AgentRL：多任务多轮 Agent 训练框架"></a>4.3 AgentRL：多任务多轮 Agent 训练框架</h3><p><strong>AgentRL</strong>（清华大学 THUDM，2025.10）是目前最系统的 Agentic RL 训练框架，其训练成果已应用于智谱的 AutoGLM。</p>
<p><strong>两大技术创新：</strong></p>
<p><strong>① 跨策略采样（Cross-Policy Sampling）</strong><br>在多轮设定中，Agent 容易陷入策略过拟合，不愿探索新策略。AgentRL 通过从多个模型策略池中采样动作，增强探索多样性：</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sqf">轨迹生成时：<br><span class="hljs-built_in">Step</span> <span class="hljs-number">1</span>: 从 Policy_A 采样 <span class="hljs-built_in">action</span><br><span class="hljs-built_in">Step</span> <span class="hljs-number">2</span>: 从 Policy_B 采样 <span class="hljs-built_in">action</span>  ← 跨策略<br><span class="hljs-built_in">Step</span> <span class="hljs-number">3</span>: 从 Policy_A 采样 <span class="hljs-built_in">action</span><br>...<br></code></pre></td></tr></table></figure>

<p><strong>② 任务优势归一化（Task Advantage Normalization）</strong><br>多任务训练时不同任务的奖励尺度差异大。对每个任务的优势值独立归一化，稳定训练：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">Advantage_normalized</span> = (Advantage - mean_task) / std_task<br></code></pre></td></tr></table></figure>

<p><strong>实验结果惊人：</strong> AgentRL 在五个 Agent 基准任务（ALFWorld、DB、KG、OS、Webshop）上训练开源 LLM（Qwen2.5），性能显著超越 GPT-5、Claude-Sonnet-4 和 DeepSeek-R1。</p>
<h3 id="4-4-DeepResearcher：真实环境中的-RL-训练"><a href="#4-4-DeepResearcher：真实环境中的-RL-训练" class="headerlink" title="4.4 DeepResearcher：真实环境中的 RL 训练"></a>4.4 DeepResearcher：真实环境中的 RL 训练</h3><p><strong>DeepResearcher</strong>（上海交通大学 GAIR，2025.4，EMNLP 2025 收录）是首个在真实 Web 搜索环境中端到端训练 Agent 的框架。</p>
<p><strong>为什么 RAG 环境训练不够？</strong></p>
<p>之前的 RL 训练工作（如 Search-R1、R1-Searcher）都在 RAG 环境中进行——给模型一个固定语料库，模型从中检索信息。这种方式有一个致命假设：<strong>所有需要的信息已经在语料库里了</strong>。但现实世界不是这样的：</p>
<ul>
<li>信息可能不存在于语料库中</li>
<li>信息可能已经过时</li>
<li>需要跨多个领域综合多个来源</li>
<li>网页格式杂乱，充满噪声和反爬机制</li>
</ul>
<p>DeepResearcher 直接在开放互联网环境中训练，Agent 需要面对真实的搜索引擎、真实的网页（包括乱码、广告、反爬）。</p>
<p><strong>多 Agent 架构设计：</strong></p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs sqf">Main <span class="hljs-built_in">Agent</span>（推理决策者）<br>    │<br>    ├── 决定搜索什么关键词<br>    ├── 分析搜索结果摘要<br>    ├── 决定深入哪些网页<br>    │       ↓<br>    └── Browsing <span class="hljs-built_in">Agent</span>（网页浏览者）<br>            ├── 加载完整网页<br>            ├── 提取相关信息<br>            └── 返回结构化摘要给 Main <span class="hljs-built_in">Agent</span><br></code></pre></td></tr></table></figure>

<p>Main Agent 负责高层决策（搜什么、看哪个、如何综合），Browsing Agent 负责底层信息提取——这比 RAG 系统”直接返回文本片段”要灵活得多。</p>
<p><strong>训练后涌现的四种认知行为（最惊喜的发现）：</strong></p>
<ol>
<li><p><strong>自主规划（Planning）</strong>：Agent 自动学会了在开始研究前制定计划，并在过程中动态调整。注意——<strong>没有人教它规划</strong>，这是纯 RL 训练涌现的行为！甚至还会”合并步骤”来提高效率。</p>
</li>
<li><p><strong>交叉验证（Cross-Validation）</strong>：Agent 找到一个答案后，不会立即接受，而是继续搜索其他来源来验证。这种”不轻信第一个结果”的审慎行为也是自发涌现的。</p>
</li>
<li><p><strong>自我反思与重定向（Self-Reflection）</strong>：发现当前搜索方向不对时，Agent 会主动调整关键词或换一个完全不同的搜索策略。</p>
</li>
<li><p><strong>诚实性（Honesty）</strong>：当确实找不到确定答案时，Agent 会坦诚说明，而非编造一个看似合理的答案。</p>
</li>
</ol>
<p><strong>量化结果：</strong> DeepResearcher 在 7 个开放域研究数据集上，比提示工程方案提升高达 <strong>28.9 分</strong>，比 RAG 环境 RL 方案提升 <strong>7.2 分</strong>。这证明了一个核心结论：<strong>在真实环境中训练不是可选的优化，而是开发稳健研究能力的根本需求。</strong></p>
<h3 id="4-5-通义-DeepResearch：全栈-Agent-训练流水线"><a href="#4-5-通义-DeepResearch：全栈-Agent-训练流水线" class="headerlink" title="4.5 通义 DeepResearch：全栈 Agent 训练流水线"></a>4.5 通义 DeepResearch：全栈 Agent 训练流水线</h3><p>阿里通义团队（2025.9）提出了一套完整的 Agent 训练流程：</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">阶段1: Agentic Pre-training（预训练阶段引入工具使用能力）</span><br>    ↓<br><span class="hljs-section">阶段2: Supervised Fine-tuning（用专家数据冷启动）</span><br>    ↓<br><span class="hljs-section">阶段3: On-policy RL（在线强化学习自进化）</span><br></code></pre></td></tr></table></figure>

<p>这套「预训练 → SFT → RL」的三阶段流程被验证为训练 Deep Research Agent 的有效范式。</p>
<hr>
<h2 id="五、推理时搜索：第三条路径"><a href="#五、推理时搜索：第三条路径" class="headerlink" title="五、推理时搜索：第三条路径"></a>五、推理时搜索：第三条路径</h2><p>前四节讨论了两条提升 Agent 能力的路径：</p>
<ul>
<li><strong>路径 A：推理时工程</strong>（更好的框架、搜索算法、记忆管理）</li>
<li><strong>路径 B：训练时学习</strong>（通过 RL 直接提升模型能力）</li>
</ul>
<p>还有<strong>路径 C</strong>——<strong>在推理时投入更多计算</strong>。不改变模型权重，也不依赖复杂的外部框架，而是让模型”想更久”。</p>
<h3 id="5-1-推理时计算扩展（Inference-Time-Scaling）"><a href="#5-1-推理时计算扩展（Inference-Time-Scaling）" class="headerlink" title="5.1 推理时计算扩展（Inference-Time Scaling）"></a>5.1 推理时计算扩展（Inference-Time Scaling）</h3><p>OpenAI 的 o1&#x2F;o3&#x2F;o4 系列揭示了一个令人振奋的发现：<strong>推理时的计算量和推理质量之间存在近似对数线性的正相关关系</strong>。换句话说，让模型多花 10 倍算力”思考”，可以获得显著的质量提升。</p>
<p>核心技术手段包括：</p>
<ul>
<li><strong>内部思维链（Internal CoT）</strong>：模型在输出前进行长链隐式推理，这些推理 token 消耗计算但不一定展示给用户</li>
<li><strong>搜索与回溯</strong>：多条推理路径并行探索，选择最优路径——本质上和 ToT 异曲同工，但被编码进了模型的推理行为中</li>
<li><strong>自我验证</strong>：模型生成候选答案后，自己检查答案的正确性，如有问题则重新推理</li>
<li><strong>自适应计算分配</strong>：简单问题少想，复杂问题多想——模型学会了”量力而行”</li>
</ul>
<p>这解释了为什么 o3&#x2F;o4 在某些数学竞赛题上表现惊人——它们可能在单个问题上花费了相当于普通模型数百次调用的算力。</p>
<h3 id="5-2-AB-MCTS：自适应分支蒙特卡洛树搜索"><a href="#5-2-AB-MCTS：自适应分支蒙特卡洛树搜索" class="headerlink" title="5.2 AB-MCTS：自适应分支蒙特卡洛树搜索"></a>5.2 AB-MCTS：自适应分支蒙特卡洛树搜索</h3><p><strong>AB-MCTS（Adaptive Branching MCTS）</strong>（Sakana AI）将这个思想推向了多模型协作的维度：</p>
<p><strong>核心思想：</strong> 不是一个模型自己搜索，而是<strong>多个不同的 LLM 协作进行蒙特卡洛树搜索</strong>。每个模型有不同的偏好和盲点，多模型搜索可以获得更全面的探索覆盖。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css">搜索树的每个节点：<br>├── GPT-<span class="hljs-number">5</span> 生成候选 <span class="hljs-selector-tag">A</span> → 评分 <span class="hljs-number">0.7</span><br>├── Claude 生成候选 <span class="hljs-selector-tag">B</span> → 评分 <span class="hljs-number">0.9</span>  ← 选中展开<br>└── Gemini 生成候选 C → 评分 <span class="hljs-number">0.5</span><br><br>下一层继续多模型扩展...<br></code></pre></td></tr></table></figure>

<p><strong>自适应分支</strong> 是关键创新：不固定每个节点的分支数，而是根据当前问题的难度和搜索进展动态调整。简单部分少分支快速通过，困难部分多分支深度探索。</p>
<p>AB-MCTS 代表了推理时搜索的前沿方向：不改变任何模型的权重，而是通过更聪明的搜索编排来突破单模型的能力上限。这和下棋中的思想完全一致——棋手的水平（模型能力）是固定的，但花更多时间思考（搜索更多变化）总能下出更好的棋。</p>
<h3 id="5-3-三条路径的关系"><a href="#5-3-三条路径的关系" class="headerlink" title="5.3 三条路径的关系"></a>5.3 三条路径的关系</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs sqf">         <span class="hljs-built_in">Agent</span> 能力提升<br>        /       |       \<br>  路径 A        路径 B        路径 C<br>推理时工程    训练时学习    推理时计算<br>(ReAct,ToT,   (Agentic RL,  (o1-style,<br> Deep <span class="hljs-built_in">Agent</span>)   <span class="hljs-built_in">Agent</span>-R1)     AB-MCTS)<br>     ↓            ↓            ↓<br>模型不变,       模型变强,     模型不变,<br>外部框架优化    内化能力      多花算力思考<br></code></pre></td></tr></table></figure>

<p>实践中，三条路径并不互斥——最强的 Agent 系统同时使用了全部三条：强 RL 训练的基座模型（路径 B）+ 推理时多步搜索（路径 C）+ 外部记忆和工具管理（路径 A）。</p>
<hr>
<h2 id="六、实用建议：如何选择-Agent-架构"><a href="#六、实用建议：如何选择-Agent-架构" class="headerlink" title="六、实用建议：如何选择 Agent 架构"></a>六、实用建议：如何选择 Agent 架构</h2><h3 id="任务复杂度-vs-架构选择"><a href="#任务复杂度-vs-架构选择" class="headerlink" title="任务复杂度 vs 架构选择"></a>任务复杂度 vs 架构选择</h3><table>
<thead>
<tr>
<th>任务类型</th>
<th>推荐架构</th>
<th>代表方案</th>
</tr>
</thead>
<tbody><tr>
<td>简单工具调用（天气&#x2F;搜索）</td>
<td>ReAct</td>
<td>LangChain ReAct Agent</td>
</tr>
<tr>
<td>多步骤有序任务</td>
<td>Plan-and-Execute</td>
<td>LangGraph</td>
</tr>
<tr>
<td>需要探索的复杂推理</td>
<td>Tree-of-Thoughts &#x2F; LATS</td>
<td>自定义</td>
</tr>
<tr>
<td>需要从失败中学习</td>
<td>Reflexion</td>
<td>自定义</td>
</tr>
<tr>
<td>超长任务（100+ 步）</td>
<td>Deep Agent</td>
<td>Claude Code &#x2F; OpenClaw</td>
</tr>
<tr>
<td>训练专用 Agent</td>
<td>Agentic RL</td>
<td>AgentRL &#x2F; Agent-R1</td>
</tr>
<tr>
<td>深度研究</td>
<td>Deep Agent + RL</td>
<td>DeepResearcher</td>
</tr>
</tbody></table>
<h3 id="关键设计原则"><a href="#关键设计原则" class="headerlink" title="关键设计原则"></a>关键设计原则</h3><ol>
<li><strong>外部化一切状态</strong>：不要仅依赖上下文窗口，用文件系统持久化记忆</li>
<li><strong>分层委派</strong>：复杂任务拆分给专门的 Sub-Agent</li>
<li><strong>显式管理上下文</strong>：主动摘要和压缩，保持高信噪比</li>
<li><strong>建立检查点机制</strong>：允许 Agent 回溯和恢复</li>
<li><strong>过程奖励优于结果奖励</strong>：在训练中引入中间步骤的奖励信号</li>
</ol>
<hr>
<h2 id="七、展望：Agent-技术的下一步"><a href="#七、展望：Agent-技术的下一步" class="headerlink" title="七、展望：Agent 技术的下一步"></a>七、展望：Agent 技术的下一步</h2><h3 id="2026-年关键趋势"><a href="#2026-年关键趋势" class="headerlink" title="2026 年关键趋势"></a>2026 年关键趋势</h3><ol>
<li><strong>Agentic RL 成为标配</strong>：从提示工程走向端到端训练，直接优化 Agent 的多步决策能力</li>
<li><strong>Memory 成为一等公民</strong>：ICLR 2026 专门设立 MemAgents Workshop，记忆管理从「工程技巧」升级为核心研究方向</li>
<li><strong>多模态 Agent</strong>：Agent 不再限于文本交互，可以「看」屏幕、操作 UI、理解视觉信息</li>
<li><strong>自进化 Agent</strong>：Agent 能在部署后持续从真实交互中学习改进</li>
</ol>
<h3 id="核心挑战"><a href="#核心挑战" class="headerlink" title="核心挑战"></a>核心挑战</h3><ul>
<li><strong>安全与对齐</strong>：自主度越高，风险越大，如何确保 Agent 行为安全可控</li>
<li><strong>长期记忆</strong>：如何在超长任务中维持一致的目标和上下文</li>
<li><strong>奖励设计</strong>：复杂任务的奖励信号如何定义和分解</li>
<li><strong>评估基准</strong>：缺乏真正长时间跨度的 Agent 评估标准</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>Yao, S. et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ICLR 2023.</li>
<li>Shinn, N. et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.” NeurIPS 2023.</li>
<li>Wei, J. et al. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” NeurIPS 2022.</li>
<li>Yao, S. et al. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” NeurIPS 2023.</li>
<li>Zhou, A. et al. “Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models.” ICML 2024.</li>
<li>Cheng, M. et al. “Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning.” arXiv:2511.14460, Nov 2025.</li>
<li>Zhang, H. et al. “AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework.” arXiv:2510.04206, Oct 2025.</li>
<li>Zheng, Y. et al. “DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments.” arXiv:2504.03160, Apr 2025.</li>
<li>Wang, R. et al. “A Practitioner’s Guide to Multi-turn Agentic Reinforcement Learning.” arXiv:2510.01132, Oct 2025.</li>
<li>“Don’t Lose the Thread: Empowering Long-Horizon LLM Agents with Cognitive Resource Self-Allocation (CORAL).” ICLR 2026.</li>
<li>“The Landscape of Agentic Reinforcement Learning for LLMs: A Survey.” TMLR, Jan 2026.</li>
<li>Tongyi Team. “Tongyi DeepResearch: A New Era of Open-Source AI Researchers.” Sep 2025.</li>
<li>OpenAI. “Introducing Deep Research.” Feb-Jul 2025.</li>
<li>Sakana AI. “Inference-Time Scaling and Collective Intelligence for Frontier AI (AB-MCTS).” 2025.</li>
</ol>
<hr>
<blockquote>
<p>本文系统梳理了 Agent Loop 和 Agent RL 领域的核心算法和最新进展，从经典的 ReAct 循环到前沿的 Agentic RL 训练范式。Agent 技术正在从「工程拼接」走向「端到端学习」，这将是 2026 年 AI 领域最重要的技术方向之一。</p>
</blockquote>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI-%E5%89%8D%E6%B2%BF/" class="category-chain-item">AI 前沿</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="print-no-link">#强化学习</a>
      
        <a href="/tags/AI-Agent/" class="print-no-link">#AI Agent</a>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6/" class="print-no-link">#深度研究</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</div>
      <div>https://hydraxman.github.io/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Nathan</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2026年2月13日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-cc-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-cc-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2026/02/14/large-scale-codebase-analysis-2026/" title="大规模代码库分析与理解：从AST到AI Agent的技术全景">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">大规模代码库分析与理解：从AST到AI Agent的技术全景</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2026/02/13/github-trending/2026-02-13/" title="GitHub Trending 热榜 | 2026-02-13：Generative UI、个人 AI 基础设施、Chrome DevTools MCP">
                        <span class="hidden-mobile">GitHub Trending 热榜 | 2026-02-13：Generative UI、个人 AI 基础设施、Chrome DevTools MCP</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  



  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <br> <span>© 2024 - 2026 内森淼文</span> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
