<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="计算机技术, 生活中的胡思乱想">
    
    <meta name="author" content="Nathan">
    
    <!-- Completely eliminate flash of wrong theme -->
    <script>
        (function() {
            const THEME_KEY = "REDEFINE-THEME-STATUS";
            const DARK = "dark", LIGHT = "light";
            
            // Get preferred theme
            function getTheme() {
                try {
                    const saved = localStorage.getItem(THEME_KEY);
                    if (saved) {
                        const { isDark } = JSON.parse(saved);
                        return isDark ? DARK : LIGHT;
                    }
                } catch (e) {}
                
                return matchMedia("(prefers-color-scheme: dark)").matches ? DARK : LIGHT;
            }
            
            // Apply theme to document
            function applyTheme(theme) {
                const isDark = theme === DARK;
                const root = document.documentElement;
                
                // Set data attribute for CSS variables
                root.setAttribute("data-theme", theme);
                
                // Set classes for compatibility
                root.classList.add(theme);
                root.classList.remove(isDark ? LIGHT : DARK);
                root.style.colorScheme = theme;
            }
            
            // Initial application
            const theme = getTheme();
            applyTheme(theme);
            
            // Listen for system preference changes
            matchMedia("(prefers-color-scheme: dark)").addEventListener("change", ({ matches }) => {
                // Only update if using system preference (no localStorage entry)
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(matches ? DARK : LIGHT);
                }
            });
            
            // Set body classes once DOM is ready
            if (document.readyState !== "loading") {
                document.body.classList.add(theme + "-mode");
            } else {
                document.addEventListener("DOMContentLoaded", () => {
                    document.body.classList.add(theme + "-mode");
                    document.body.classList.remove((theme === DARK ? LIGHT : DARK) + "-mode");
                });
            }
        })();
    </script>
    
    <!-- Critical CSS to prevent flash -->
    <style>
        :root[data-theme="dark"] {
            --background-color: #202124;
            --background-color-transparent: rgba(32, 33, 36, 0.6);
            --second-background-color: #2d2e32;
            --third-background-color: #34353a;
            --third-background-color-transparent: rgba(32, 33, 36, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #ffffff;
            --second-text-color: #eeeeee;
            --third-text-color: #bebec6;
            --fourth-text-color: #999999;
            --default-text-color: #bebec6;
            --invert-text-color: #373D3F;
            --border-color: rgba(255, 255, 255, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(255, 255, 255, 0.08);
            --shadow-color-2: rgba(255, 255, 255, 0.05);
        }
        
        :root[data-theme="light"] {
            --background-color: #fff;
            --background-color-transparent: rgba(255, 255, 255, 0.6);
            --second-background-color: #f8f8f8;
            --third-background-color: #f2f2f2;
            --third-background-color-transparent: rgba(241, 241, 241, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #16171a;
            --second-text-color: #2f3037;
            --third-text-color: #5e5e5e;
            --fourth-text-color: #eeeeee;
            --default-text-color: #373D3F;
            --invert-text-color: #bebec6;
            --border-color: rgba(0, 0, 0, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(0, 0, 0, 0.08);
            --shadow-color-2: rgba(0, 0, 0, 0.05);
        }
        
        body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
        
        /* Apply body classes as soon as DOM is ready */
        :root[data-theme="dark"] body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
    </style>
    
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://hydraxman.github.io/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景">
<meta property="og:url" content="https://hydraxman.github.io/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/index.html">
<meta property="og:site_name" content="内森淼文">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hydraxman.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2026-02-13T13:30:00.000Z">
<meta property="article:modified_time" content="2026-02-20T09:22:12.115Z">
<meta property="article:author" content="Nathan">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="AI Agent">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="深度研究">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hydraxman.github.io/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.svg">
    <meta name="theme-color" content="#ffffff">
    <link rel="shortcut icon" href="/images/favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景 | 内森淼文
        
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    
        
            
    
            
    
            
                
                    <link rel="stylesheet" href="/css/custom.css">
                
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/tailwind.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
    
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    
    
        <link href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@400;700&display=swap" rel="stylesheet">
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"hydraxman.github.io","root":"/","language":"zh-CN","path":"search.json"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":false,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#ffffff","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":true,"family":"Inter","url":"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap"},"title":{"enable":true,"family":"Noto Serif","url":"https://fonts.googleapis.com/css2?family=Noto+Serif:wght@400;700&display=swap"}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":true,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":false,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"side_tools":{"gear_rotation":true,"auto_expand":false},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/banner-light.jpg","dark":"/images/banner-dark.jpg"},"title":"内森淼文","subtitle":{"text":["AI · 编程 · 科技见闻","Write Code, Read World"],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/hydraxman","instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.5","navbar":{"auto_hide":false,"color":{"left":"#333333","right":"#555555","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"About":{"path":"/about","icon":"fa-regular fa-user"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"AI · 编程 · 科技","show_on_mobile":true,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2024/01/01 00:00:00"};
    window.lang_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>



<body>
	<div class="progress-bar-container">
	
	<span class="scroll-progress-bar"></span>
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                内森淼文
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    首页
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    归档
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/categories"
                                        >
                                    <i class="fa-regular fa-folder fa-fw"></i>
                                    分类
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/tags"
                                        >
                                    <i class="fa-regular fa-tags fa-fw"></i>
                                    标签
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/about"
                                        >
                                    <i class="fa-regular fa-user fa-fw"></i>
                                    关于
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i>
                    </div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                首页
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                归档
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/categories"
                        >
                            <span>
                                分类
                            </span>
                            
                                <i class="fa-regular fa-folder fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/tags"
                        >
                            <span>
                                标签
                            </span>
                            
                                <i class="fa-regular fa-tags fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/about"
                        >
                            <span>
                                关于
                            </span>
                            
                                <i class="fa-regular fa-user fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
                
                    
                    
                    
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">115</div>
        <div class="label text-third-text-color text-sm">标签</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">5</div>
        <div class="label text-third-text-color text-sm">分类</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">54</div>
        <div class="label text-third-text-color text-sm">文章</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/avatar.png">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">Nathan</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2026-02-13 21:30</span>
        <span class="mobile">2026-02-13 21:30</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2026-02-20 17:22:12</span>
            <span class="mobile">2026-02-20 17:22:12</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/AI-%E5%89%8D%E6%B2%BF/">AI 前沿</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/AI-Agent/">AI Agent</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/LLM/">LLM</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6/">深度研究</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>8.1k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>30 分钟</span>
        </span>
    
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<h2 id="引言：从聊天机器人到自主智能体"><a href="#引言：从聊天机器人到自主智能体" class="headerlink" title="引言：从聊天机器人到自主智能体"></a>引言：从聊天机器人到自主智能体</h2><p>2025-2026 年，AI Agent 迎来了从「对话助手」到「自主执行者」的质变。过去，构建一个 AI Agent 的方法极其简单——拿一个大语言模型（LLM），套一个 while 循环，给它接上工具 API，就能完成简单任务。但当任务变得复杂（比如深度研究、代码重构、多步决策），这种朴素架构就会崩溃。</p>
<p>Agent 领域正在经历一场深刻的范式转移：<strong>从基于提示工程的静态 Agent，走向基于强化学习的自适应 Agent</strong>。本文将系统梳理驱动 Agent 完成长任务的各类算法与架构，从经典的 Agent Loop 到前沿的 Agentic RL。</p>
<hr>
<h2 id="一、Agent-Loop：基础循环架构"><a href="#一、Agent-Loop：基础循环架构" class="headerlink" title="一、Agent Loop：基础循环架构"></a>一、Agent Loop：基础循环架构</h2><h3 id="1-1-最简-Agent-循环"><a href="#1-1-最简-Agent-循环" class="headerlink" title="1.1 最简 Agent 循环"></a>1.1 最简 Agent 循环</h3><p>最基础的 Agent 架构可以抽象为一个循环：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while not done:</span><br><span class="line">    thought = LLM.think(context)</span><br><span class="line">    action = LLM.decide(thought)</span><br><span class="line">    observation = environment.execute(action)</span><br><span class="line">    context.append(observation)</span><br></pre></td></tr></table></figure></div>

<p>这就是所谓的 <strong>Agent 1.0 架构</strong>。LLM 充当「大脑」，在循环中反复执行「思考→行动→观察」直到任务完成或达到终止条件。</p>
<h3 id="1-2-ReAct：推理与行动的交错"><a href="#1-2-ReAct：推理与行动的交错" class="headerlink" title="1.2 ReAct：推理与行动的交错"></a>1.2 ReAct：推理与行动的交错</h3><p><strong>ReAct（Reasoning and Acting）</strong> 是最经典的 Agent Loop 框架，由 Yao 等人于 2022 年提出。其核心思想是让 LLM 交替生成推理步骤和执行动作：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">循环流程：</span><br><span class="line">Thought → Action → Observation → Thought → Action → Observation → ... → Final Answer</span><br></pre></td></tr></table></figure></div>

<p><strong>关键设计：</strong></p>
<ul>
<li><strong>Thought（思考）</strong>：LLM 内部推理，分析当前状态，制定下一步计划</li>
<li><strong>Action（行动）</strong>：调用外部工具（搜索、计算、API 等）</li>
<li><strong>Observation（观察）</strong>：接收工具返回结果，更新上下文</li>
</ul>
<p>ReAct 的优势在于推理过程透明可追踪，但在长任务中会遇到严重问题：上下文窗口被大量历史信息污染，导致模型「迷失方向」。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/agent-loop-react.png"
                      alt="Agent Loop ReAct 架构"
                ><br><em>图：ReAct 的核心循环——Thought（推理）→ Action（行动）→ Observation（观察）交替执行，直到任务完成。</em></p>
<h3 id="1-3-Plan-and-Execute：先规划后执行"><a href="#1-3-Plan-and-Execute：先规划后执行" class="headerlink" title="1.3 Plan-and-Execute：先规划后执行"></a>1.3 Plan-and-Execute：先规划后执行</h3><p>为解决 ReAct 在长任务中的漂移问题，<strong>Plan-and-Execute</strong> 架构将任务分为两个阶段：</p>
<ol>
<li><strong>规划阶段（Planner）</strong>：LLM 分析任务，生成分步计划</li>
<li><strong>执行阶段（Executor）</strong>：按计划逐步执行，每步可调用工具</li>
<li><strong>重规划（Re-Planner）</strong>：根据执行结果动态调整剩余计划</li>
</ol>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Plan: [Step1, Step2, Step3, Step4]</span><br><span class="line">Execute Step1 → Result1</span><br><span class="line">Re-Plan: [Step2&#x27;, Step3, Step4]  // 根据 Result1 调整</span><br><span class="line">Execute Step2&#x27; → Result2</span><br><span class="line">...</span><br></pre></td></tr></table></figure></div>

<p>这种架构的优势是目标感更强，不容易在长链推理中丧失方向。</p>
<h3 id="1-4-ReWOO：推理与观察解耦"><a href="#1-4-ReWOO：推理与观察解耦" class="headerlink" title="1.4 ReWOO：推理与观察解耦"></a>1.4 ReWOO：推理与观察解耦</h3><p><strong>ReWOO（Reasoning Without Observation）</strong> 进一步优化了 Plan-and-Execute 模式：</p>
<ul>
<li>先一次性生成完整的推理计划和所有工具调用</li>
<li>并行执行所有工具调用</li>
<li>最后综合所有结果生成答案</li>
</ul>
<p>优势是减少 LLM 调用次数，提升效率；但牺牲了动态调整能力。</p>
<hr>
<h2 id="二、高级推理框架：从线性到树状再到自我进化"><a href="#二、高级推理框架：从线性到树状再到自我进化" class="headerlink" title="二、高级推理框架：从线性到树状再到自我进化"></a>二、高级推理框架：从线性到树状再到自我进化</h2><p>上一节的 Agent Loop 架构解决了”如何让 LLM 与外部世界交互”的问题，但它们都面临一个共同瓶颈：<strong>推理质量</strong>。ReAct 按顺序执行每一步，一旦某步方向错误，整个链条就会偏离正确路径。Plan-and-Execute 有规划能力，但规划本身也可能出错，且无法在推理层面进行深度探索。</p>
<p>这就引出了一个核心问题：<strong>如何让 LLM 在推理过程中更智能地搜索和探索？</strong></p>
<p>答案隐藏在三个递进的范式中：CoT（线性思考）→ ToT（树状探索）→ Reflexion&#x2F;LATS（自我进化的搜索）。</p>
<h3 id="2-1-Chain-of-Thought（CoT）：思维链推理"><a href="#2-1-Chain-of-Thought（CoT）：思维链推理" class="headerlink" title="2.1 Chain-of-Thought（CoT）：思维链推理"></a>2.1 Chain-of-Thought（CoT）：思维链推理</h3><p>CoT 是 Agent 推理的基石。2022 年 Wei 等人在 Google Brain 的工作揭示了一个关键发现：只需在提示中加入”Let’s think step by step”，就能让 LLM 将复杂问题拆解为一系列中间推理步骤，显著提升数学、逻辑和常识推理的准确率。</p>
<p><strong>CoT 的核心机制：</strong></p>
<ul>
<li>LLM 不再直接输出最终答案，而是生成一条<strong>线性推理链</strong></li>
<li>每个中间步骤为后续步骤提供额外的”证据”或约束条件</li>
<li>从概率角度看，这相当于对模型输出分布的贝叶斯更新——每一步都缩小了解空间</li>
</ul>
<p><strong>一个典型例子：</strong></p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">问题：停车场有3排车，每排8辆，又开走了4辆，还剩多少辆？</span><br><span class="line"></span><br><span class="line">CoT 推理链：</span><br><span class="line">Step 1: 总共有 3 × 8 = 24 辆车</span><br><span class="line">Step 2: 开走了 4 辆</span><br><span class="line">Step 3: 剩下 24 - 4 = 20 辆</span><br><span class="line">答案：20 辆</span><br></pre></td></tr></table></figure></div>

<p>然而，CoT 作为一条<strong>单一的线性链条</strong>，存在三个根本性局限：</p>
<p><strong>① 不可回溯（No Backtracking）</strong><br>一旦某一步推理出错，错误会沿着链条一路传播，无法返回修正。就像在迷宫中只能一直往前走，走错了也不能回头。</p>
<p><strong>② 无法探索多条路径（No Branching）</strong><br>面对有多种可能解法的问题，CoT 只能选择一条路走到底。比如解数学题时，可能有代数法和几何法两条路径，CoT 只会选择一条。</p>
<p><strong>③ 缺乏自我评估（No Self-Evaluation）</strong><br>链条上的每一步都没有被评估是否合理，模型无法判断当前方向是否正确，只能盲目前进。</p>
<p>CoT-SC（Self-Consistency）通过<strong>并行采样多条独立链条</strong>然后投票选择最佳答案，部分缓解了上述问题——但每条链条之间仍然是完全独立的，无法共享中间发现，也无法在关键决策点分叉探索。</p>
<h3 id="2-2-从-CoT-到-ToT：为什么需要进化？"><a href="#2-2-从-CoT-到-ToT：为什么需要进化？" class="headerlink" title="2.2 从 CoT 到 ToT：为什么需要进化？"></a>2.2 从 CoT 到 ToT：为什么需要进化？</h3><p>正是 CoT 的上述三个局限催生了 Tree-of-Thoughts（ToT）。让我们通过一个经典问题来理解这个进化的必要性：</p>
<p><strong>Game of 24 问题</strong>：用 4、5、6、10 四个数字和加减乘除，组合出结果为 24。</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CoT 的困境：</span><br><span class="line">Step 1: 尝试 4 × 6 = 24... 但还剩 5 和 10 没用</span><br><span class="line">Step 2: 好像不行，但已经无法回头了</span><br><span class="line">→ 失败，只能从头再来一次（但新的一次完全独立，不会记住上次的教训）</span><br><span class="line"></span><br><span class="line">ToT 的优势：</span><br><span class="line">          4, 5, 6, 10</span><br><span class="line">        /      |       \</span><br><span class="line">   4+5=9    4×6=24    10-6=4</span><br><span class="line">   /    \     ✗(剩余无法凑)  /   \</span><br><span class="line"> 9×(10-6)  6×(10-5)   4×5=20  ...</span><br><span class="line"> =9×4=36✗  =6×5=30✗   20+4=24? ✗(4已用)</span><br><span class="line">                        ↑ 回溯，换路</span><br></pre></td></tr></table></figure></div>

<p>ToT 在每一步都可以<strong>分叉探索多个方向</strong>，在发现死路时<strong>回溯</strong>到之前的节点尝试其他路径。</p>
<h3 id="2-3-Tree-of-Thoughts（ToT）：树状搜索推理"><a href="#2-3-Tree-of-Thoughts（ToT）：树状搜索推理" class="headerlink" title="2.3 Tree-of-Thoughts（ToT）：树状搜索推理"></a>2.3 Tree-of-Thoughts（ToT）：树状搜索推理</h3><p>ToT（由 Yao 等人于 2023 年在普林斯顿提出）将推理从线性链扩展为<strong>树状结构</strong>，本质上是将经典搜索算法引入 LLM 推理过程。</p>
<p><strong>ToT 的四大核心组件：</strong></p>
<p><strong>① 思维分解（Thought Decomposition）</strong><br>将问题拆解为适当粒度的”思维单元”。粒度选择至关重要——太细则搜索空间爆炸，太粗则失去探索灵活性。比如写一篇文章，一个思维单元可以是”段落大纲”而非”单个句子”。</p>
<p><strong>② 思维生成（Thought Generation）</strong><br>在每个节点生成 k 个候选思维，有两种策略：</p>
<ul>
<li><strong>采样（Sample）</strong>：独立生成多个候选（适合创意性任务，解空间大）</li>
<li><strong>提议（Propose）</strong>：基于前文依次生成（适合逻辑性任务，避免重复）</li>
</ul>
<p><strong>③ 状态评估（State Evaluation）</strong><br>这是 ToT 最关键的创新——<strong>用 LLM 自己来评估每个中间状态的质量</strong>：</p>
<ul>
<li><strong>打分法</strong>：对每个状态评分（如 1-10 分，或”确定&#x2F;可能&#x2F;不可能”）</li>
<li><strong>投票法</strong>：让 LLM 比较多个候选，选出最有前途的</li>
</ul>
<p>评估函数充当了”导航仪”的角色，告诉搜索算法哪些方向值得继续探索、哪些应该剪枝放弃。</p>
<p><strong>④ 搜索算法（Search Algorithm）</strong><br>ToT 支持两种经典搜索策略：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BFS（广度优先）：                    DFS（深度优先）：</span><br><span class="line">层层扩展，不遗漏                     一条路走到底，走不通再回头</span><br><span class="line"></span><br><span class="line">Level 0:    [A]                     探索顺序：A → B → D → (回溯) → E → (回溯)</span><br><span class="line">Level 1:  [B] [C]                              → C → F → ✓ 找到解</span><br><span class="line">Level 2: [D][E][F][G]</span><br><span class="line"></span><br><span class="line">适合：解空间较小，需要最优解           适合：解空间大，需要尽快找到一个可行解</span><br></pre></td></tr></table></figure></div>

<p><strong>ToT 解决了 CoT 的三大痛点：</strong></p>
<table>
<thead>
<tr>
<th>CoT 的局限</th>
<th>ToT 的解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>不可回溯</td>
<td>DFS 自然支持回溯，发现死路可返回上一节点</td>
</tr>
<tr>
<td>无法分叉探索</td>
<td>每个节点可生成 k 个候选分支</td>
</tr>
<tr>
<td>缺乏自我评估</td>
<td>状态评估函数在每步进行质量判断</td>
</tr>
</tbody></table>
<p><strong>代价是什么？</strong> ToT 需要更多的 LLM 调用（生成 + 评估），计算成本显著高于 CoT。这是”搜索质量”与”计算成本”之间的经典权衡——和 AlphaGo 的蒙特卡洛树搜索是同一种思想。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/cot-vs-tot-comparison.png"
                      alt="CoT vs ToT 推理结构对比"
                ><br><em>图：CoT 线性链 vs ToT 树状搜索的结构差异。CoT 只能沿单一路径前进，ToT 在每步分叉并评估，支持回溯探索。</em></p>
<h3 id="2-4-Reflexion：自我反思学习"><a href="#2-4-Reflexion：自我反思学习" class="headerlink" title="2.4 Reflexion：自我反思学习"></a>2.4 Reflexion：自我反思学习</h3><p>ToT 解决了”单次推理中的探索问题”，但还有一个更深层的问题没有解决：<strong>跨任务的经验积累</strong>。人类在失败后会反思总结教训，下次遇到类似问题时表现更好。CoT 和 ToT 都没有这种”从失败中学习”的能力——每次推理都从零开始。</p>
<p><strong>Reflexion</strong> 引入了一个关键的「反思」闭环，让 Agent 能在多次尝试之间积累经验：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Trial 1: Actor 执行 → 得到结果 → Evaluator 评判 → 失败 ❌</span><br><span class="line">         ↓</span><br><span class="line">    Self-Reflection: &quot;我在第3步选错了API，应该用search而不是lookup&quot;</span><br><span class="line">         ↓  反思摘要存入长期记忆</span><br><span class="line">Trial 2: Actor 带着反思记忆重新执行 → 改进但仍有问题 ❌</span><br><span class="line">         ↓</span><br><span class="line">    Self-Reflection: &quot;API调对了，但参数格式不对，应该用JSON而非字符串&quot;</span><br><span class="line">         ↓  追加到长期记忆</span><br><span class="line">Trial 3: Actor 综合两次教训执行 → 成功 ✅</span><br></pre></td></tr></table></figure></div>

<p>Reflexion 包含三个核心组件的协作循环：</p>
<ul>
<li><strong>Actor（执行器）</strong>：基于 ReAct 或 CoT 的执行引擎，负责实际操作</li>
<li><strong>Evaluator（评估器）</strong>：判断执行结果的成功&#x2F;失败，提供二元或标量反馈信号</li>
<li><strong>Self-Reflection（反思器）</strong>：最核心的创新——将失败经验转化为自然语言反思摘要，存入一个持久的<strong>语言记忆（verbal memory）</strong></li>
</ul>
<p><strong>为什么用语言记忆而不是梯度更新？</strong> 这是 Reflexion 最巧妙的设计——传统 RL 通过更新模型权重来学习，成本极高且需要大量样本。Reflexion 用自然语言存储教训（如”不要用 deprecated API v1，改用 v2”），轻量、可解释，而且在推理时通过上下文注入即可使用。</p>
<p><strong>Reflexion 在编程任务上的惊人效果：</strong></p>
<ul>
<li>HumanEval 上从 80.1%（CoT 基线）提升到 91.0%（+11%）</li>
<li>其中约 40% 的错误在第二次尝试时就被修正</li>
<li>这证明了”反思+重试”机制的强大——很多错误不需要更强的模型，只需要从失败中学到教训</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/reflexion-learning-loop.png"
                      alt="Reflexion 自我反思学习循环"
                ><br><em>图：Reflexion 的三组件反思循环——Actor 执行、Evaluator 评判、Self-Reflection 生成语言化教训存入记忆，驱动下一次尝试改进。</em></p>
<h3 id="2-5-从-ToT-到-LATS：统一搜索、行动与反思"><a href="#2-5-从-ToT-到-LATS：统一搜索、行动与反思" class="headerlink" title="2.5 从 ToT 到 LATS：统一搜索、行动与反思"></a>2.5 从 ToT 到 LATS：统一搜索、行动与反思</h3><p>到这里，我们已经有了三种关键能力：</p>
<ul>
<li><strong>CoT&#x2F;ToT</strong>：推理时的搜索与探索</li>
<li><strong>ReAct</strong>：与外部环境的交互（工具调用）</li>
<li><strong>Reflexion</strong>：从失败中学习</li>
</ul>
<p>但这三种能力是<strong>各自独立</strong>的。ToT 只做推理搜索，不调用工具；ReAct 调用工具但不做搜索；Reflexion 做反思但搜索策略很原始。有没有一个框架能把这三者统一起来？</p>
<p><strong>LATS（Language Agent Tree Search）</strong> 正是这个统一框架。它将蒙特卡洛树搜索（MCTS）——AlphaGo 的核心算法——引入 LLM Agent 决策，将搜索、行动和反思融合为一个整体：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">                   ┌──────────────────────┐</span><br><span class="line">                   │    MCTS 搜索循环      │</span><br><span class="line">                   └──────────────────────┘</span><br><span class="line">                             │</span><br><span class="line">   ┌─────────────┬──────────┼──────────┬──────────────┐</span><br><span class="line">   ▼             ▼          ▼          ▼              ▼</span><br><span class="line">Selection    Expansion  Simulation  Evaluation   Backpropagation</span><br><span class="line">(选择节点)   (生成动作)  (执行动作)  (LLM评分)    (反向传播)</span><br><span class="line">   │             │          │          │              │</span><br><span class="line"> UCB1算法    LLM生成     调用工具    价值评估      更新节点</span><br><span class="line"> 平衡探索     候选动作    获取反馈    + 反思        质量分数</span><br><span class="line"> 与利用                              </span><br></pre></td></tr></table></figure></div>

<p><strong>MCTS 五步循环详解：</strong></p>
<p><strong>① Selection（选择）</strong>：用 UCB1 公式选择最值得探索的节点，自动平衡”深入已知好路径”和”尝试未探索方向”——这解决了 ToT 简单 BFS&#x2F;DFS 策略的效率问题。</p>
<p><strong>② Expansion（扩展）</strong>：在选中节点用 LLM 生成 n 个候选动作，每个动作可以是推理步骤或工具调用——这融合了 ToT 的分支能力和 ReAct 的工具交互。</p>
<p><strong>③ Simulation（模拟）</strong>：执行动作并观察环境反馈——ReAct 的核心循环。</p>
<p><strong>④ Evaluation（评估）</strong>：LLM 对当前状态进行价值评估，给出分数。<strong>关键创新：如果检测到失败，触发 Reflexion 式的自我反思，生成反思摘要指导后续搜索。</strong></p>
<p><strong>⑤ Backpropagation（反向传播）</strong>：将评估分数沿树路径回传，更新每个祖先节点的质量估计——这让 LATS 能从全局视角优化搜索方向。</p>
<p><strong>LATS &#x3D; ToT（搜索框架）+ ReAct（环境交互）+ Reflexion（失败学习）</strong></p>
<p>这种统一带来的效果是显著的：在 HotPotQA 多跳推理任务上，LATS 比单独的 ReAct 提升 16%，比 Reflexion 提升 8%，比 ToT 提升 12%。代价是更高的计算成本——但这正是”用推理时计算换取更好决策”的核心思想。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/lats-mcts-agent.png"
                      alt="LATS 蒙特卡洛树搜索 Agent 决策"
                ><br><em>图：LATS 将 MCTS 的五步循环应用于 Agent 决策，统一了搜索（ToT）、行动（ReAct）和反思（Reflexion）三大能力。</em></p>
<hr>
<h2 id="三、Deep-Agent-架构：当任务跨越百步"><a href="#三、Deep-Agent-架构：当任务跨越百步" class="headerlink" title="三、Deep Agent 架构：当任务跨越百步"></a>三、Deep Agent 架构：当任务跨越百步</h2><p>LATS 统一了搜索、行动和反思，但它仍然在<strong>单个上下文窗口</strong>内运作。当任务复杂度从”几步完成”升级到”数十甚至上百步”——比如写一份完整的研究报告、重构一个大型代码库、或执行一个跨越数小时的深度研究——上下文窗口就成了不可逾越的瓶颈：推理历史、工具返回、中间结果……全部挤在有限的 token 窗口中，信噪比急剧下降。</p>
<p><strong>Deep Agent（深度智能体）</strong> 架构是 2025 年下半年兴起的新范式，代表产品包括 Claude Code、OpenAI Deep Research、Manus AI 等。它的核心思想是：**将 Agent 的认知从”上下文内”扩展到”上下文外”**——用外部持久化系统弥补上下文窗口的局限。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/agent-architecture-evolution.png"
                      alt="Agent 架构演进全景图"
                ><br><em>图：从 ReAct 到 Deep Agent 的架构演进。每一步进化都在解决前一代的核心瓶颈：ReAct 解决了工具交互、CoT&#x2F;ToT 解决了推理搜索、Reflexion 解决了经验学习、Deep Agent 解决了长任务上下文管理。</em></p>
<h3 id="3-1-四大支柱"><a href="#3-1-四大支柱" class="headerlink" title="3.1 四大支柱"></a>3.1 四大支柱</h3><p>Deep Agent 架构建立在四个基础之上：</p>
<p><strong>① 显式规划（Explicit Planning）</strong><br>不依赖 LLM 隐式推理，而是维护一个<strong>外部的、可持久化的任务计划</strong>。计划可以被检查、修改和恢复。这意味着即使上下文窗口被清空，Agent 仍然知道自己在做什么、做到了哪一步。</p>
<p>以 Claude Code 为例：当它重构一个大型代码库时，会在文件系统中写入一份 <code>plan.md</code>，记录每个模块的改造状态。即使中间因为上下文溢出导致会话重置，Agent 读取 plan.md 后就能无缝继续。</p>
<p><strong>② 层级委派（Hierarchical Delegation）</strong><br>单个 Agent 的能力总有上限。Deep Agent 将复杂任务拆分给<strong>专门化的子 Agent</strong>，每个子 Agent 有独立的上下文窗口和专属工具集：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Orchestrator Agent（指挥官）</span><br><span class="line">├── Research Sub-Agent（负责信息检索）→ 有搜索工具</span><br><span class="line">├── Code Sub-Agent（负责代码编写）→ 有文件读写和终端</span><br><span class="line">├── Review Sub-Agent（负责质量审查）→ 有测试工具</span><br><span class="line">└── Memory Sub-Agent（负责信息管理）→ 有知识库</span><br></pre></td></tr></table></figure></div>

<p>这种设计的关键优势是<strong>上下文隔离</strong>：Research Agent 的搜索结果不会污染 Code Agent 的编码上下文。每个子 Agent 只接收与自己任务相关的信息，信噪比大幅提升。</p>
<p><strong>③ 持久化记忆（Persistent Memory）</strong><br>使用文件系统作为外部记忆，而非仅依赖上下文窗口。Agent 可以：</p>
<ul>
<li>写入结构化笔记和发现摘要</li>
<li>读取之前的研究结果</li>
<li>维护状态文件跟踪进度</li>
<li>建立知识索引便于快速检索</li>
</ul>
<p>这本质上是将人类研究员的”做笔记”习惯编码为 Agent 的核心行为——上下文窗口是”工作记忆”（短期），文件系统是”笔记本”（长期）。</p>
<p><strong>④ 极致的上下文工程（Extreme Context Engineering）</strong><br>精心管理什么信息进入上下文窗口。具体技术包括：</p>
<ul>
<li><strong>渐进式摘要</strong>：每隔 N 步将历史压缩为摘要</li>
<li><strong>选择性加载</strong>：只加载与当前子任务相关的信息</li>
<li><strong>上下文分层</strong>：系统提示 &gt; 当前任务 &gt; 相关历史 &gt; 可选参考</li>
<li><strong>智能截断</strong>：工具返回过长时自动截取关键部分</li>
</ul>
<p>以 OpenClaw 为例——它在每次 heartbeat 时读取 HEARTBEAT.md（而非全部历史），在每个 session 开始时读取 SOUL.md 和 USER.md（身份信息），只有在主 session 中才加载 MEMORY.md（长期记忆），这就是上下文工程的实际应用。</p>
<h3 id="3-2-CORAL：认知资源自分配"><a href="#3-2-CORAL：认知资源自分配" class="headerlink" title="3.2 CORAL：认知资源自分配"></a>3.2 CORAL：认知资源自分配</h3><p><strong>CORAL（Cognitive Resource Self-Allocation）</strong> 是 ICLR 2026 收录的工作，专门解决长任务中 Agent 的「注意力漂移」问题——当上下文中积累了太多无关信息，LLM 的注意力被分散，推理质量急剧下降。</p>
<p><strong>核心洞察：</strong> 人类处理长任务时，会主动”清空短期记忆”——比如写论文写了3小时后，会先休息，回来后重新读一遍大纲，而不是试图记住之前的每一个细节。CORAL 给 Agent 提供了类似的能力。</p>
<p><strong>工作记忆管理工具集：</strong></p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Agent 工具箱中新增三种&quot;元工具&quot;：</span><br><span class="line">1. set_checkpoint(label)  → 在关键节点保存状态快照</span><br><span class="line">2. clear_memory()         → 清除工作记忆中的杂乱信息  </span><br><span class="line">3. restore(label)         → 从指定检查点恢复推理上下文</span><br></pre></td></tr></table></figure></div>

<p>当 Agent 探索了多条路径、积累了大量搜索结果，发现自己”迷失方向”时，可以主动调用 <code>clear_memory()</code> + <code>restore(&quot;initial_plan&quot;)</code> 来重新开始——但不是完全从零开始，而是保留了检查点中的关键发现。</p>
<p><strong>训练方式：</strong> CORAL 使用<strong>多轮 Agentic 强化策略优化（Multi-episode Agentic Reinforced Policy Optimization）</strong> 算法，让 Agent 通过 RL 学会三个关键判断：</p>
<ul>
<li><strong>何时设置检查点</strong>（在做出重要发现或关键决策后）</li>
<li><strong>何时清理记忆</strong>（当上下文信噪比过低时）</li>
<li><strong>恢复到哪个检查点</strong>（选择最有价值的历史状态）</li>
</ul>
<p>CORAL 在 SWE-bench 等长任务基准上显著优于没有记忆管理的 Agent，验证了”主动管理认知资源”的价值——这也为 Deep Agent 架构的”极致上下文工程”提供了理论支撑。</p>
<hr>
<h2 id="四、Agentic-RL：从工程拼接到端到端学习"><a href="#四、Agentic-RL：从工程拼接到端到端学习" class="headerlink" title="四、Agentic RL：从工程拼接到端到端学习"></a>四、Agentic RL：从工程拼接到端到端学习</h2><p>前三节的所有架构——从 ReAct 到 LATS 到 Deep Agent——都属于「推理时（inference-time）」的工程技巧。它们通过精巧的提示设计、搜索算法和记忆管理来提升 Agent 表现，但<strong>模型本身并没有因此变得更强</strong>。模型权重是冻结的，所有的”聪明”都来自外部框架。</p>
<p>这就像给一个普通人配备了最好的工具箱、最详细的操作手册——他确实能完成更复杂的任务，但他自身的能力并没有提升。如果工具箱被拿走或手册不适用，他就回到了原点。</p>
<p><strong>Agentic RL</strong> 代表了一个根本性的范式转移：<strong>直接通过强化学习训练 LLM 的 Agent 行为能力</strong>，让模型在多步交互中学会规划、工具使用、错误修正——这些能力被编码进模型权重，而非依赖外部框架。</p>
<h3 id="4-1-从-RLHF-到-Agentic-RL"><a href="#4-1-从-RLHF-到-Agentic-RL" class="headerlink" title="4.1 从 RLHF 到 Agentic RL"></a>4.1 从 RLHF 到 Agentic RL</h3><table>
<thead>
<tr>
<th>维度</th>
<th>RLHF</th>
<th>Agentic RL</th>
</tr>
</thead>
<tbody><tr>
<td>目标</td>
<td>让 LLM 输出更符合人类偏好</td>
<td>让 LLM 学会多步决策与工具使用</td>
</tr>
<tr>
<td>交互</td>
<td>单轮：prompt → response</td>
<td>多轮：action → env feedback → action</td>
</tr>
<tr>
<td>奖励</td>
<td>人类偏好评分</td>
<td>任务完成度 + 过程奖励</td>
</tr>
<tr>
<td>训练格式</td>
<td>单条序列</td>
<td>多轮轨迹（trajectory）</td>
</tr>
<tr>
<td>环境</td>
<td>无</td>
<td>真实或模拟环境</td>
</tr>
</tbody></table>
<p>Agentic RL 的核心突破在于：<strong>将 LLM 从被动的序列生成器重新定义为主动的、嵌入复杂动态世界的决策智能体。</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/agentic-rl-paradigm.png"
                      alt="Agentic RL 范式转移"
                ><br><em>图：从推理时工程（左）到 Agentic RL 端到端训练（右）的范式转移——外部脚手架 vs 内化能力。</em></p>
<h3 id="4-2-Agent-R1：端到端-Agent-强化学习"><a href="#4-2-Agent-R1：端到端-Agent-强化学习" class="headerlink" title="4.2 Agent-R1：端到端 Agent 强化学习"></a>4.2 Agent-R1：端到端 Agent 强化学习</h3><p><strong>Agent-R1</strong>（中国科学技术大学，2025.11）是将 DeepSeek-R1 的 RL 训练范式扩展到 Agent 场景的里程碑工作。</p>
<p><strong>核心问题：为什么不能直接把 RLHF&#x2F;GRPO 套到 Agent 上？</strong></p>
<p>传统 RL 训练 LLM 时，轨迹（trajectory）是一个单轮序列：prompt → response。但 Agent 的轨迹是多轮交互序列，包含两种本质不同的 token：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">轨迹结构：</span><br><span class="line">[System Prompt] → Agent生成思考 → Agent调用工具 → 环境返回结果 → Agent继续思考 → ...</span><br><span class="line">                  ↑ Agent token（可训练）      ↑ 环境 token（不可训练！）</span><br></pre></td></tr></table></figure></div>

<p>关键区分：<strong>Agent 生成的 token 需要参与梯度计算，但环境返回的 token 不应该</strong>——因为你不能通过训练模型来改变环境的行为。Agent-R1 在 MDP 框架中明确建模了这个区分。</p>
<p><strong>MDP 扩展详解：</strong></p>
<table>
<thead>
<tr>
<th>组件</th>
<th>静态 LLM</th>
<th>Agent-R1</th>
</tr>
</thead>
<tbody><tr>
<td><strong>状态空间</strong></td>
<td>prompt + 已生成 token</td>
<td>完整对话历史 + 每轮环境反馈</td>
</tr>
<tr>
<td><strong>动作空间</strong></td>
<td>词表中选下一个 token</td>
<td>同上，但 token 序列可触发工具调用</td>
</tr>
<tr>
<td><strong>转移函数</strong></td>
<td>确定性（拼接 token）</td>
<td><strong>随机性</strong>（环境返回不确定）</td>
</tr>
<tr>
<td><strong>奖励函数</strong></td>
<td>单次终端奖励</td>
<td>终端奖励 + 中间过程奖励</td>
</tr>
</tbody></table>
<p><strong>过程奖励（Process Rewards）</strong> 是 Agent-R1 的重要创新——不只在任务完成时给奖励，在中间步骤也给信号。比如：正确调用了 search API 但查询词不够精确，可以给一个小的正奖励（鼓励工具使用）但不是满分（查询还需优化）。这解决了长任务中”奖励稀疏”的经典难题。</p>
<p>Agent-R1 开源了完整的训练框架（基于 veRL），支持快速接入不同环境，已在 Multi-hop QA 上验证了效果。</p>
<h3 id="4-3-AgentRL：多任务多轮-Agent-训练框架"><a href="#4-3-AgentRL：多任务多轮-Agent-训练框架" class="headerlink" title="4.3 AgentRL：多任务多轮 Agent 训练框架"></a>4.3 AgentRL：多任务多轮 Agent 训练框架</h3><p><strong>AgentRL</strong>（清华大学 THUDM，2025.10）是目前最系统的 Agentic RL 训练框架，其训练成果已应用于智谱的 AutoGLM。</p>
<p><strong>两大技术创新：</strong></p>
<p><strong>① 跨策略采样（Cross-Policy Sampling）</strong><br>在多轮设定中，Agent 容易陷入策略过拟合，不愿探索新策略。AgentRL 通过从多个模型策略池中采样动作，增强探索多样性：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">轨迹生成时：</span><br><span class="line">Step 1: 从 Policy_A 采样 action</span><br><span class="line">Step 2: 从 Policy_B 采样 action  ← 跨策略</span><br><span class="line">Step 3: 从 Policy_A 采样 action</span><br><span class="line">...</span><br></pre></td></tr></table></figure></div>

<p><strong>② 任务优势归一化（Task Advantage Normalization）</strong><br>多任务训练时不同任务的奖励尺度差异大。对每个任务的优势值独立归一化，稳定训练：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Advantage_normalized = (Advantage - mean_task) / std_task</span><br></pre></td></tr></table></figure></div>

<p><strong>实验结果惊人：</strong> AgentRL 在五个 Agent 基准任务（ALFWorld、DB、KG、OS、Webshop）上训练开源 LLM（Qwen2.5），性能显著超越 GPT-5、Claude-Sonnet-4 和 DeepSeek-R1。</p>
<h3 id="4-4-DeepResearcher：真实环境中的-RL-训练"><a href="#4-4-DeepResearcher：真实环境中的-RL-训练" class="headerlink" title="4.4 DeepResearcher：真实环境中的 RL 训练"></a>4.4 DeepResearcher：真实环境中的 RL 训练</h3><p><strong>DeepResearcher</strong>（上海交通大学 GAIR，2025.4，EMNLP 2025 收录）是首个在真实 Web 搜索环境中端到端训练 Agent 的框架。</p>
<p><strong>为什么 RAG 环境训练不够？</strong></p>
<p>之前的 RL 训练工作（如 Search-R1、R1-Searcher）都在 RAG 环境中进行——给模型一个固定语料库，模型从中检索信息。这种方式有一个致命假设：<strong>所有需要的信息已经在语料库里了</strong>。但现实世界不是这样的：</p>
<ul>
<li>信息可能不存在于语料库中</li>
<li>信息可能已经过时</li>
<li>需要跨多个领域综合多个来源</li>
<li>网页格式杂乱，充满噪声和反爬机制</li>
</ul>
<p>DeepResearcher 直接在开放互联网环境中训练，Agent 需要面对真实的搜索引擎、真实的网页（包括乱码、广告、反爬）。</p>
<p><strong>多 Agent 架构设计：</strong></p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Main Agent（推理决策者）</span><br><span class="line">    │</span><br><span class="line">    ├── 决定搜索什么关键词</span><br><span class="line">    ├── 分析搜索结果摘要</span><br><span class="line">    ├── 决定深入哪些网页</span><br><span class="line">    │       ↓</span><br><span class="line">    └── Browsing Agent（网页浏览者）</span><br><span class="line">            ├── 加载完整网页</span><br><span class="line">            ├── 提取相关信息</span><br><span class="line">            └── 返回结构化摘要给 Main Agent</span><br></pre></td></tr></table></figure></div>

<p>Main Agent 负责高层决策（搜什么、看哪个、如何综合），Browsing Agent 负责底层信息提取——这比 RAG 系统”直接返回文本片段”要灵活得多。</p>
<p><strong>训练后涌现的四种认知行为（最惊喜的发现）：</strong></p>
<ol>
<li><p><strong>自主规划（Planning）</strong>：Agent 自动学会了在开始研究前制定计划，并在过程中动态调整。注意——<strong>没有人教它规划</strong>，这是纯 RL 训练涌现的行为！甚至还会”合并步骤”来提高效率。</p>
</li>
<li><p><strong>交叉验证（Cross-Validation）</strong>：Agent 找到一个答案后，不会立即接受，而是继续搜索其他来源来验证。这种”不轻信第一个结果”的审慎行为也是自发涌现的。</p>
</li>
<li><p><strong>自我反思与重定向（Self-Reflection）</strong>：发现当前搜索方向不对时，Agent 会主动调整关键词或换一个完全不同的搜索策略。</p>
</li>
<li><p><strong>诚实性（Honesty）</strong>：当确实找不到确定答案时，Agent 会坦诚说明，而非编造一个看似合理的答案。</p>
</li>
</ol>
<p><strong>量化结果：</strong> DeepResearcher 在 7 个开放域研究数据集上，比提示工程方案提升高达 <strong>28.9 分</strong>，比 RAG 环境 RL 方案提升 <strong>7.2 分</strong>。这证明了一个核心结论：<strong>在真实环境中训练不是可选的优化，而是开发稳健研究能力的根本需求。</strong></p>
<h3 id="4-5-通义-DeepResearch：全栈-Agent-训练流水线"><a href="#4-5-通义-DeepResearch：全栈-Agent-训练流水线" class="headerlink" title="4.5 通义 DeepResearch：全栈 Agent 训练流水线"></a>4.5 通义 DeepResearch：全栈 Agent 训练流水线</h3><p>阿里通义团队（2025.9）提出了一套完整的 Agent 训练流程：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">阶段1: Agentic Pre-training（预训练阶段引入工具使用能力）</span><br><span class="line">    ↓</span><br><span class="line">阶段2: Supervised Fine-tuning（用专家数据冷启动）</span><br><span class="line">    ↓</span><br><span class="line">阶段3: On-policy RL（在线强化学习自进化）</span><br></pre></td></tr></table></figure></div>

<p>这套「预训练 → SFT → RL」的三阶段流程被验证为训练 Deep Research Agent 的有效范式。</p>
<hr>
<h2 id="五、推理时搜索：第三条路径"><a href="#五、推理时搜索：第三条路径" class="headerlink" title="五、推理时搜索：第三条路径"></a>五、推理时搜索：第三条路径</h2><p>前四节讨论了两条提升 Agent 能力的路径：</p>
<ul>
<li><strong>路径 A：推理时工程</strong>（更好的框架、搜索算法、记忆管理）</li>
<li><strong>路径 B：训练时学习</strong>（通过 RL 直接提升模型能力）</li>
</ul>
<p>还有<strong>路径 C</strong>——<strong>在推理时投入更多计算</strong>。不改变模型权重，也不依赖复杂的外部框架，而是让模型”想更久”。</p>
<h3 id="5-1-推理时计算扩展（Inference-Time-Scaling）"><a href="#5-1-推理时计算扩展（Inference-Time-Scaling）" class="headerlink" title="5.1 推理时计算扩展（Inference-Time Scaling）"></a>5.1 推理时计算扩展（Inference-Time Scaling）</h3><p>OpenAI 的 o1&#x2F;o3&#x2F;o4 系列揭示了一个令人振奋的发现：<strong>推理时的计算量和推理质量之间存在近似对数线性的正相关关系</strong>。换句话说，让模型多花 10 倍算力”思考”，可以获得显著的质量提升。</p>
<p>核心技术手段包括：</p>
<ul>
<li><strong>内部思维链（Internal CoT）</strong>：模型在输出前进行长链隐式推理，这些推理 token 消耗计算但不一定展示给用户</li>
<li><strong>搜索与回溯</strong>：多条推理路径并行探索，选择最优路径——本质上和 ToT 异曲同工，但被编码进了模型的推理行为中</li>
<li><strong>自我验证</strong>：模型生成候选答案后，自己检查答案的正确性，如有问题则重新推理</li>
<li><strong>自适应计算分配</strong>：简单问题少想，复杂问题多想——模型学会了”量力而行”</li>
</ul>
<p>这解释了为什么 o3&#x2F;o4 在某些数学竞赛题上表现惊人——它们可能在单个问题上花费了相当于普通模型数百次调用的算力。</p>
<h3 id="5-2-AB-MCTS：自适应分支蒙特卡洛树搜索"><a href="#5-2-AB-MCTS：自适应分支蒙特卡洛树搜索" class="headerlink" title="5.2 AB-MCTS：自适应分支蒙特卡洛树搜索"></a>5.2 AB-MCTS：自适应分支蒙特卡洛树搜索</h3><p><strong>AB-MCTS（Adaptive Branching MCTS）</strong>（Sakana AI）将这个思想推向了多模型协作的维度：</p>
<p><strong>核心思想：</strong> 不是一个模型自己搜索，而是<strong>多个不同的 LLM 协作进行蒙特卡洛树搜索</strong>。每个模型有不同的偏好和盲点，多模型搜索可以获得更全面的探索覆盖。</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">搜索树的每个节点：</span><br><span class="line">├── GPT-5 生成候选 A → 评分 0.7</span><br><span class="line">├── Claude 生成候选 B → 评分 0.9  ← 选中展开</span><br><span class="line">└── Gemini 生成候选 C → 评分 0.5</span><br><span class="line"></span><br><span class="line">下一层继续多模型扩展...</span><br></pre></td></tr></table></figure></div>

<p><strong>自适应分支</strong> 是关键创新：不固定每个节点的分支数，而是根据当前问题的难度和搜索进展动态调整。简单部分少分支快速通过，困难部分多分支深度探索。</p>
<p>AB-MCTS 代表了推理时搜索的前沿方向：不改变任何模型的权重，而是通过更聪明的搜索编排来突破单模型的能力上限。这和下棋中的思想完全一致——棋手的水平（模型能力）是固定的，但花更多时间思考（搜索更多变化）总能下出更好的棋。</p>
<h3 id="5-3-三条路径的关系"><a href="#5-3-三条路径的关系" class="headerlink" title="5.3 三条路径的关系"></a>5.3 三条路径的关系</h3><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">         Agent 能力提升</span><br><span class="line">        /       |       \</span><br><span class="line">  路径 A        路径 B        路径 C</span><br><span class="line">推理时工程    训练时学习    推理时计算</span><br><span class="line">(ReAct,ToT,   (Agentic RL,  (o1-style,</span><br><span class="line"> Deep Agent)   Agent-R1)     AB-MCTS)</span><br><span class="line">     ↓            ↓            ↓</span><br><span class="line">模型不变,       模型变强,     模型不变,</span><br><span class="line">外部框架优化    内化能力      多花算力思考</span><br></pre></td></tr></table></figure></div>

<p>实践中，三条路径并不互斥——最强的 Agent 系统同时使用了全部三条：强 RL 训练的基座模型（路径 B）+ 推理时多步搜索（路径 C）+ 外部记忆和工具管理（路径 A）。</p>
<hr>
<h2 id="六、实用建议：如何选择-Agent-架构"><a href="#六、实用建议：如何选择-Agent-架构" class="headerlink" title="六、实用建议：如何选择 Agent 架构"></a>六、实用建议：如何选择 Agent 架构</h2><h3 id="任务复杂度-vs-架构选择"><a href="#任务复杂度-vs-架构选择" class="headerlink" title="任务复杂度 vs 架构选择"></a>任务复杂度 vs 架构选择</h3><table>
<thead>
<tr>
<th>任务类型</th>
<th>推荐架构</th>
<th>代表方案</th>
</tr>
</thead>
<tbody><tr>
<td>简单工具调用（天气&#x2F;搜索）</td>
<td>ReAct</td>
<td>LangChain ReAct Agent</td>
</tr>
<tr>
<td>多步骤有序任务</td>
<td>Plan-and-Execute</td>
<td>LangGraph</td>
</tr>
<tr>
<td>需要探索的复杂推理</td>
<td>Tree-of-Thoughts &#x2F; LATS</td>
<td>自定义</td>
</tr>
<tr>
<td>需要从失败中学习</td>
<td>Reflexion</td>
<td>自定义</td>
</tr>
<tr>
<td>超长任务（100+ 步）</td>
<td>Deep Agent</td>
<td>Claude Code &#x2F; OpenClaw</td>
</tr>
<tr>
<td>训练专用 Agent</td>
<td>Agentic RL</td>
<td>AgentRL &#x2F; Agent-R1</td>
</tr>
<tr>
<td>深度研究</td>
<td>Deep Agent + RL</td>
<td>DeepResearcher</td>
</tr>
</tbody></table>
<h3 id="关键设计原则"><a href="#关键设计原则" class="headerlink" title="关键设计原则"></a>关键设计原则</h3><ol>
<li><strong>外部化一切状态</strong>：不要仅依赖上下文窗口，用文件系统持久化记忆</li>
<li><strong>分层委派</strong>：复杂任务拆分给专门的 Sub-Agent</li>
<li><strong>显式管理上下文</strong>：主动摘要和压缩，保持高信噪比</li>
<li><strong>建立检查点机制</strong>：允许 Agent 回溯和恢复</li>
<li><strong>过程奖励优于结果奖励</strong>：在训练中引入中间步骤的奖励信号</li>
</ol>
<hr>
<h2 id="七、展望：Agent-技术的下一步"><a href="#七、展望：Agent-技术的下一步" class="headerlink" title="七、展望：Agent 技术的下一步"></a>七、展望：Agent 技术的下一步</h2><h3 id="2026-年关键趋势"><a href="#2026-年关键趋势" class="headerlink" title="2026 年关键趋势"></a>2026 年关键趋势</h3><ol>
<li><strong>Agentic RL 成为标配</strong>：从提示工程走向端到端训练，直接优化 Agent 的多步决策能力</li>
<li><strong>Memory 成为一等公民</strong>：ICLR 2026 专门设立 MemAgents Workshop，记忆管理从「工程技巧」升级为核心研究方向</li>
<li><strong>多模态 Agent</strong>：Agent 不再限于文本交互，可以「看」屏幕、操作 UI、理解视觉信息</li>
<li><strong>自进化 Agent</strong>：Agent 能在部署后持续从真实交互中学习改进</li>
</ol>
<h3 id="核心挑战"><a href="#核心挑战" class="headerlink" title="核心挑战"></a>核心挑战</h3><ul>
<li><strong>安全与对齐</strong>：自主度越高，风险越大，如何确保 Agent 行为安全可控</li>
<li><strong>长期记忆</strong>：如何在超长任务中维持一致的目标和上下文</li>
<li><strong>奖励设计</strong>：复杂任务的奖励信号如何定义和分解</li>
<li><strong>评估基准</strong>：缺乏真正长时间跨度的 Agent 评估标准</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>Yao, S. et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ICLR 2023.</li>
<li>Shinn, N. et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.” NeurIPS 2023.</li>
<li>Wei, J. et al. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” NeurIPS 2022.</li>
<li>Yao, S. et al. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” NeurIPS 2023.</li>
<li>Zhou, A. et al. “Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models.” ICML 2024.</li>
<li>Cheng, M. et al. “Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning.” arXiv:2511.14460, Nov 2025.</li>
<li>Zhang, H. et al. “AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework.” arXiv:2510.04206, Oct 2025.</li>
<li>Zheng, Y. et al. “DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments.” arXiv:2504.03160, Apr 2025.</li>
<li>Wang, R. et al. “A Practitioner’s Guide to Multi-turn Agentic Reinforcement Learning.” arXiv:2510.01132, Oct 2025.</li>
<li>“Don’t Lose the Thread: Empowering Long-Horizon LLM Agents with Cognitive Resource Self-Allocation (CORAL).” ICLR 2026.</li>
<li>“The Landscape of Agentic Reinforcement Learning for LLMs: A Survey.” TMLR, Jan 2026.</li>
<li>Tongyi Team. “Tongyi DeepResearch: A New Era of Open-Source AI Researchers.” Sep 2025.</li>
<li>OpenAI. “Introducing Deep Research.” Feb-Jul 2025.</li>
<li>Sakana AI. “Inference-Time Scaling and Collective Intelligence for Frontier AI (AB-MCTS).” 2025.</li>
</ol>
<hr>
<blockquote>
<p>本文系统梳理了 Agent Loop 和 Agent RL 领域的核心算法和最新进展，从经典的 ReAct 循环到前沿的 Agentic RL 训练范式。Agent 技术正在从「工程拼接」走向「端到端学习」，这将是 2026 年 AI 领域最重要的技术方向之一。</p>
</blockquote>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</li>
        <li><strong>作者:</strong> Nathan</li>
        <li><strong>创建于
                :</strong> 2026-02-13 21:30:00</li>
        
            <li>
                <strong>更新于
                    :</strong> 2026-02-20 17:22:12
            </li>
        
        <li>
            <strong>链接:</strong> https://hydraxman.github.io/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/
        </li>
        <li>
            <strong>
                版权声明:
            </strong>
            

            
                本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a> 进行许可。
            
        </li>
    </ul>
</div>

		</div>
		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">#强化学习</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/tags/AI-Agent/">#AI Agent</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/tags/LLM/">#LLM</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/tags/%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6/">#深度研究</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			<div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="prev" rel="prev" href="/2026/02/14/large-scale-codebase-analysis-2026/">
					<span class="left arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-left"></i>
					</span>
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">大规模代码库分析与理解：从AST到AI Agent的技术全景</span>
						<span class="post-nav-item">上一篇</span>
					</span>
				</a>
			</div>
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2026/02/13/github-trending/2026-02-13/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">GitHub Trending 热榜 | 2026-02-13：Generative UI、个人 AI 基础设施、Chrome DevTools MCP</span>
						<span class="post-nav-item">下一篇</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">目录</div>
		<div class="page-title">Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</div>
		<ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%95%E8%A8%80%EF%BC%9A%E4%BB%8E%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%88%B0%E8%87%AA%E4%B8%BB%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-text">引言：从聊天机器人到自主智能体</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Agent-Loop%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%BE%AA%E7%8E%AF%E6%9E%B6%E6%9E%84"><span class="nav-text">一、Agent Loop：基础循环架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E6%9C%80%E7%AE%80-Agent-%E5%BE%AA%E7%8E%AF"><span class="nav-text">1.1 最简 Agent 循环</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-ReAct%EF%BC%9A%E6%8E%A8%E7%90%86%E4%B8%8E%E8%A1%8C%E5%8A%A8%E7%9A%84%E4%BA%A4%E9%94%99"><span class="nav-text">1.2 ReAct：推理与行动的交错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Plan-and-Execute%EF%BC%9A%E5%85%88%E8%A7%84%E5%88%92%E5%90%8E%E6%89%A7%E8%A1%8C"><span class="nav-text">1.3 Plan-and-Execute：先规划后执行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-ReWOO%EF%BC%9A%E6%8E%A8%E7%90%86%E4%B8%8E%E8%A7%82%E5%AF%9F%E8%A7%A3%E8%80%A6"><span class="nav-text">1.4 ReWOO：推理与观察解耦</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E9%AB%98%E7%BA%A7%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%EF%BC%9A%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E6%A0%91%E7%8A%B6%E5%86%8D%E5%88%B0%E8%87%AA%E6%88%91%E8%BF%9B%E5%8C%96"><span class="nav-text">二、高级推理框架：从线性到树状再到自我进化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Chain-of-Thought%EF%BC%88CoT%EF%BC%89%EF%BC%9A%E6%80%9D%E7%BB%B4%E9%93%BE%E6%8E%A8%E7%90%86"><span class="nav-text">2.1 Chain-of-Thought（CoT）：思维链推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E4%BB%8E-CoT-%E5%88%B0-ToT%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E8%BF%9B%E5%8C%96%EF%BC%9F"><span class="nav-text">2.2 从 CoT 到 ToT：为什么需要进化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Tree-of-Thoughts%EF%BC%88ToT%EF%BC%89%EF%BC%9A%E6%A0%91%E7%8A%B6%E6%90%9C%E7%B4%A2%E6%8E%A8%E7%90%86"><span class="nav-text">2.3 Tree-of-Thoughts（ToT）：树状搜索推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Reflexion%EF%BC%9A%E8%87%AA%E6%88%91%E5%8F%8D%E6%80%9D%E5%AD%A6%E4%B9%A0"><span class="nav-text">2.4 Reflexion：自我反思学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-%E4%BB%8E-ToT-%E5%88%B0-LATS%EF%BC%9A%E7%BB%9F%E4%B8%80%E6%90%9C%E7%B4%A2%E3%80%81%E8%A1%8C%E5%8A%A8%E4%B8%8E%E5%8F%8D%E6%80%9D"><span class="nav-text">2.5 从 ToT 到 LATS：统一搜索、行动与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Deep-Agent-%E6%9E%B6%E6%9E%84%EF%BC%9A%E5%BD%93%E4%BB%BB%E5%8A%A1%E8%B7%A8%E8%B6%8A%E7%99%BE%E6%AD%A5"><span class="nav-text">三、Deep Agent 架构：当任务跨越百步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E5%9B%9B%E5%A4%A7%E6%94%AF%E6%9F%B1"><span class="nav-text">3.1 四大支柱</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-CORAL%EF%BC%9A%E8%AE%A4%E7%9F%A5%E8%B5%84%E6%BA%90%E8%87%AA%E5%88%86%E9%85%8D"><span class="nav-text">3.2 CORAL：认知资源自分配</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Agentic-RL%EF%BC%9A%E4%BB%8E%E5%B7%A5%E7%A8%8B%E6%8B%BC%E6%8E%A5%E5%88%B0%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0"><span class="nav-text">四、Agentic RL：从工程拼接到端到端学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E4%BB%8E-RLHF-%E5%88%B0-Agentic-RL"><span class="nav-text">4.1 从 RLHF 到 Agentic RL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Agent-R1%EF%BC%9A%E7%AB%AF%E5%88%B0%E7%AB%AF-Agent-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">4.2 Agent-R1：端到端 Agent 强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-AgentRL%EF%BC%9A%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%A4%9A%E8%BD%AE-Agent-%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6"><span class="nav-text">4.3 AgentRL：多任务多轮 Agent 训练框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-DeepResearcher%EF%BC%9A%E7%9C%9F%E5%AE%9E%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84-RL-%E8%AE%AD%E7%BB%83"><span class="nav-text">4.4 DeepResearcher：真实环境中的 RL 训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-%E9%80%9A%E4%B9%89-DeepResearch%EF%BC%9A%E5%85%A8%E6%A0%88-Agent-%E8%AE%AD%E7%BB%83%E6%B5%81%E6%B0%B4%E7%BA%BF"><span class="nav-text">4.5 通义 DeepResearch：全栈 Agent 训练流水线</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E6%8E%A8%E7%90%86%E6%97%B6%E6%90%9C%E7%B4%A2%EF%BC%9A%E7%AC%AC%E4%B8%89%E6%9D%A1%E8%B7%AF%E5%BE%84"><span class="nav-text">五、推理时搜索：第三条路径</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%89%A9%E5%B1%95%EF%BC%88Inference-Time-Scaling%EF%BC%89"><span class="nav-text">5.1 推理时计算扩展（Inference-Time Scaling）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-AB-MCTS%EF%BC%9A%E8%87%AA%E9%80%82%E5%BA%94%E5%88%86%E6%94%AF%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2"><span class="nav-text">5.2 AB-MCTS：自适应分支蒙特卡洛树搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E4%B8%89%E6%9D%A1%E8%B7%AF%E5%BE%84%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">5.3 三条路径的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E5%AE%9E%E7%94%A8%E5%BB%BA%E8%AE%AE%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9-Agent-%E6%9E%B6%E6%9E%84"><span class="nav-text">六、实用建议：如何选择 Agent 架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E5%A4%8D%E6%9D%82%E5%BA%A6-vs-%E6%9E%B6%E6%9E%84%E9%80%89%E6%8B%A9"><span class="nav-text">任务复杂度 vs 架构选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99"><span class="nav-text">关键设计原则</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81%E5%B1%95%E6%9C%9B%EF%BC%9AAgent-%E6%8A%80%E6%9C%AF%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5"><span class="nav-text">七、展望：Agent 技术的下一步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2026-%E5%B9%B4%E5%85%B3%E9%94%AE%E8%B6%8B%E5%8A%BF"><span class="nav-text">2026 年关键趋势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%8C%91%E6%88%98"><span class="nav-text">核心挑战</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-text">参考文献</span></a></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2024</span>
              -
            
            2026&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #333"></i>&nbsp;&nbsp;<a href="/">Nathan</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        共撰写了 54 篇文章
                    </span>
                    
                        <span>
                            共 116.6k 字
                        </span>
                    
                </p>
            
        </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span>
            <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span>
        </div>
        
        
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
            
                
        
                
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	
	<div class="search-pop-overlay">
	<div class="popup search-popup">
		<div class="search-header">
			<span class="search-input-field-pre">
				<i class="fa-solid fa-keyboard"></i>
			</span>
			<div class="search-input-container">
				<input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="站内搜索您需要的内容..." spellcheck="false" type="search" class="search-input">
			</div>
			<span class="popup-btn-close">
				<i class="fa-solid fa-times"></i>
			</span>
		</div>
		<div id="search-result">
			<div id="no-result">
				<i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
			</div>
		</div>
	</div>
</div>
	

</main>



<script src="/js/build/libs/Swup.min.js"></script>

<script src="/js/build/libs/SwupSlideTheme.min.js"></script>

<script src="/js/build/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/build/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/build/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/build/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	
<script src="/js/build/tools/imageViewer.js" type="module"></script>

<script src="/js/build/utils.js" type="module"></script>

<script src="/js/build/main.js" type="module"></script>

<script src="/js/build/layouts/navbarShrink.js" type="module"></script>

<script src="/js/build/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/build/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/build/layouts/categoryList.js" type="module"></script>



    
<script src="/js/build/tools/localSearch.js" type="module"></script>




    
<script src="/js/build/tools/codeBlock.js" type="module"></script>




    
<script src="/js/build/layouts/lazyload.js" type="module"></script>




    
<script src="/js/build/tools/runtime.js"></script>

    
<script src="/js/build/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/build/libs/Typed.min.js"></script>

  
<script src="/js/build/plugins/typed.js" type="module"></script>








    
<script src="/js/build/libs/anime.min.js"></script>





    
<script src="/js/build/tools/tocToggle.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/layouts/toc.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/plugins/tabs.js" type="module" data-swup-reload-script=""></script>




<script src="/js/build/libs/moment-with-locales.min.js" data-swup-reload-script=""></script>


<script src="/js/build/layouts/essays.js" type="module" data-swup-reload-script=""></script>





	
</body>

</html>