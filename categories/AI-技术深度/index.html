<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>分类: AI 技术深度 - 内森淼文</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="内森淼文"><meta name="msapplication-TileImage" content="/images/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="内森淼文"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="内森的个人水水水文输出阵地，涵盖技术、日常思考"><meta property="og:type" content="blog"><meta property="og:title" content="内森淼文"><meta property="og:url" content="https://hydraxman.github.io/"><meta property="og:site_name" content="内森淼文"><meta property="og:description" content="内森的个人水水水文输出阵地，涵盖技术、日常思考"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hydraxman.github.io/img/og_image.png"><meta property="article:author" content="Nathan"><meta property="article:tag" content="计算机技术, 生活中的胡思乱想"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://hydraxman.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hydraxman.github.io"},"headline":"内森淼文","image":["https://hydraxman.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Nathan"},"publisher":{"@type":"Organization","name":"内森淼文","logo":{"@type":"ImageObject","url":"https://hydraxman.github.io/images/logo.svg"}},"description":"内森的个人水水水文输出阵地，涵盖技术、日常思考"}</script><link rel="icon" href="/images/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo.svg" alt="内森淼文" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="My GitHub Index" href="https://github.com/hydraxman"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">AI 技术深度</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2026-02-13T13:30:00.000Z" title="2/13/2026, 9:30:00 PM">2026-02-13</time>发表</span><span class="level-item"><time dateTime="2026-02-14T00:22:28.769Z" title="2/14/2026, 8:22:28 AM">2026-02-14</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/AI-%E6%8A%80%E6%9C%AF%E6%B7%B1%E5%BA%A6/">AI 技术深度</a></span><span class="level-item">1 小时读完 (大约9144个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/13/agent-loop-and-agent-rl-algorithms-2026/">Agent Loop 与 Agent RL：驱动 AI Agent 完成长任务的算法全景</a></h1><div class="content"><h2 id="引言：从聊天机器人到自主智能体"><a href="#引言：从聊天机器人到自主智能体" class="headerlink" title="引言：从聊天机器人到自主智能体"></a>引言：从聊天机器人到自主智能体</h2><p>2025-2026 年，AI Agent 迎来了从「对话助手」到「自主执行者」的质变。过去，构建一个 AI Agent 的方法极其简单——拿一个大语言模型（LLM），套一个 while 循环，给它接上工具 API，就能完成简单任务。但当任务变得复杂（比如深度研究、代码重构、多步决策），这种朴素架构就会崩溃。</p>
<p>Agent 领域正在经历一场深刻的范式转移：<strong>从基于提示工程的静态 Agent，走向基于强化学习的自适应 Agent</strong>。本文将系统梳理驱动 Agent 完成长任务的各类算法与架构，从经典的 Agent Loop 到前沿的 Agentic RL。</p>
<hr>
<h2 id="一、Agent-Loop：基础循环架构"><a href="#一、Agent-Loop：基础循环架构" class="headerlink" title="一、Agent Loop：基础循环架构"></a>一、Agent Loop：基础循环架构</h2><h3 id="1-1-最简-Agent-循环"><a href="#1-1-最简-Agent-循环" class="headerlink" title="1.1 最简 Agent 循环"></a>1.1 最简 Agent 循环</h3><p>最基础的 Agent 架构可以抽象为一个循环：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while not done:</span><br><span class="line">    thought = LLM.think(context)</span><br><span class="line">    action = LLM.decide(thought)</span><br><span class="line">    observation = environment.execute(action)</span><br><span class="line">    context.append(observation)</span><br></pre></td></tr></table></figure>

<p>这就是所谓的 <strong>Agent 1.0 架构</strong>。LLM 充当「大脑」，在循环中反复执行「思考→行动→观察」直到任务完成或达到终止条件。</p>
<h3 id="1-2-ReAct：推理与行动的交错"><a href="#1-2-ReAct：推理与行动的交错" class="headerlink" title="1.2 ReAct：推理与行动的交错"></a>1.2 ReAct：推理与行动的交错</h3><p><strong>ReAct（Reasoning and Acting）</strong> 是最经典的 Agent Loop 框架，由 Yao 等人于 2022 年提出。其核心思想是让 LLM 交替生成推理步骤和执行动作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">循环流程：</span><br><span class="line">Thought → Action → Observation → Thought → Action → Observation → ... → Final Answer</span><br></pre></td></tr></table></figure>

<p><strong>关键设计：</strong></p>
<ul>
<li><strong>Thought（思考）</strong>：LLM 内部推理，分析当前状态，制定下一步计划</li>
<li><strong>Action（行动）</strong>：调用外部工具（搜索、计算、API 等）</li>
<li><strong>Observation（观察）</strong>：接收工具返回结果，更新上下文</li>
</ul>
<p>ReAct 的优势在于推理过程透明可追踪，但在长任务中会遇到严重问题：上下文窗口被大量历史信息污染，导致模型「迷失方向」。</p>
<p><img src="/images/agent-loop-react.png" alt="Agent Loop ReAct 架构"><br><em>图：ReAct 的核心循环——Thought（推理）→ Action（行动）→ Observation（观察）交替执行，直到任务完成。</em></p>
<h3 id="1-3-Plan-and-Execute：先规划后执行"><a href="#1-3-Plan-and-Execute：先规划后执行" class="headerlink" title="1.3 Plan-and-Execute：先规划后执行"></a>1.3 Plan-and-Execute：先规划后执行</h3><p>为解决 ReAct 在长任务中的漂移问题，<strong>Plan-and-Execute</strong> 架构将任务分为两个阶段：</p>
<ol>
<li><strong>规划阶段（Planner）</strong>：LLM 分析任务，生成分步计划</li>
<li><strong>执行阶段（Executor）</strong>：按计划逐步执行，每步可调用工具</li>
<li><strong>重规划（Re-Planner）</strong>：根据执行结果动态调整剩余计划</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Plan: [Step1, Step2, Step3, Step4]</span><br><span class="line">Execute Step1 → Result1</span><br><span class="line">Re-Plan: [Step2&#x27;, Step3, Step4]  // 根据 Result1 调整</span><br><span class="line">Execute Step2&#x27; → Result2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>这种架构的优势是目标感更强，不容易在长链推理中丧失方向。</p>
<h3 id="1-4-ReWOO：推理与观察解耦"><a href="#1-4-ReWOO：推理与观察解耦" class="headerlink" title="1.4 ReWOO：推理与观察解耦"></a>1.4 ReWOO：推理与观察解耦</h3><p><strong>ReWOO（Reasoning Without Observation）</strong> 进一步优化了 Plan-and-Execute 模式：</p>
<ul>
<li>先一次性生成完整的推理计划和所有工具调用</li>
<li>并行执行所有工具调用</li>
<li>最后综合所有结果生成答案</li>
</ul>
<p>优势是减少 LLM 调用次数，提升效率；但牺牲了动态调整能力。</p>
<hr>
<h2 id="二、高级推理框架：从线性到树状再到自我进化"><a href="#二、高级推理框架：从线性到树状再到自我进化" class="headerlink" title="二、高级推理框架：从线性到树状再到自我进化"></a>二、高级推理框架：从线性到树状再到自我进化</h2><p>上一节的 Agent Loop 架构解决了”如何让 LLM 与外部世界交互”的问题，但它们都面临一个共同瓶颈：<strong>推理质量</strong>。ReAct 按顺序执行每一步，一旦某步方向错误，整个链条就会偏离正确路径。Plan-and-Execute 有规划能力，但规划本身也可能出错，且无法在推理层面进行深度探索。</p>
<p>这就引出了一个核心问题：<strong>如何让 LLM 在推理过程中更智能地搜索和探索？</strong></p>
<p>答案隐藏在三个递进的范式中：CoT（线性思考）→ ToT（树状探索）→ Reflexion&#x2F;LATS（自我进化的搜索）。</p>
<h3 id="2-1-Chain-of-Thought（CoT）：思维链推理"><a href="#2-1-Chain-of-Thought（CoT）：思维链推理" class="headerlink" title="2.1 Chain-of-Thought（CoT）：思维链推理"></a>2.1 Chain-of-Thought（CoT）：思维链推理</h3><p>CoT 是 Agent 推理的基石。2022 年 Wei 等人在 Google Brain 的工作揭示了一个关键发现：只需在提示中加入”Let’s think step by step”，就能让 LLM 将复杂问题拆解为一系列中间推理步骤，显著提升数学、逻辑和常识推理的准确率。</p>
<p><strong>CoT 的核心机制：</strong></p>
<ul>
<li>LLM 不再直接输出最终答案，而是生成一条<strong>线性推理链</strong></li>
<li>每个中间步骤为后续步骤提供额外的”证据”或约束条件</li>
<li>从概率角度看，这相当于对模型输出分布的贝叶斯更新——每一步都缩小了解空间</li>
</ul>
<p><strong>一个典型例子：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">问题：停车场有3排车，每排8辆，又开走了4辆，还剩多少辆？</span><br><span class="line"></span><br><span class="line">CoT 推理链：</span><br><span class="line">Step 1: 总共有 3 × 8 = 24 辆车</span><br><span class="line">Step 2: 开走了 4 辆</span><br><span class="line">Step 3: 剩下 24 - 4 = 20 辆</span><br><span class="line">答案：20 辆</span><br></pre></td></tr></table></figure>

<p>然而，CoT 作为一条<strong>单一的线性链条</strong>，存在三个根本性局限：</p>
<p><strong>① 不可回溯（No Backtracking）</strong><br>一旦某一步推理出错，错误会沿着链条一路传播，无法返回修正。就像在迷宫中只能一直往前走，走错了也不能回头。</p>
<p><strong>② 无法探索多条路径（No Branching）</strong><br>面对有多种可能解法的问题，CoT 只能选择一条路走到底。比如解数学题时，可能有代数法和几何法两条路径，CoT 只会选择一条。</p>
<p><strong>③ 缺乏自我评估（No Self-Evaluation）</strong><br>链条上的每一步都没有被评估是否合理，模型无法判断当前方向是否正确，只能盲目前进。</p>
<p>CoT-SC（Self-Consistency）通过<strong>并行采样多条独立链条</strong>然后投票选择最佳答案，部分缓解了上述问题——但每条链条之间仍然是完全独立的，无法共享中间发现，也无法在关键决策点分叉探索。</p>
<h3 id="2-2-从-CoT-到-ToT：为什么需要进化？"><a href="#2-2-从-CoT-到-ToT：为什么需要进化？" class="headerlink" title="2.2 从 CoT 到 ToT：为什么需要进化？"></a>2.2 从 CoT 到 ToT：为什么需要进化？</h3><p>正是 CoT 的上述三个局限催生了 Tree-of-Thoughts（ToT）。让我们通过一个经典问题来理解这个进化的必要性：</p>
<p><strong>Game of 24 问题</strong>：用 4、5、6、10 四个数字和加减乘除，组合出结果为 24。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CoT 的困境：</span><br><span class="line">Step 1: 尝试 4 × 6 = 24... 但还剩 5 和 10 没用</span><br><span class="line">Step 2: 好像不行，但已经无法回头了</span><br><span class="line">→ 失败，只能从头再来一次（但新的一次完全独立，不会记住上次的教训）</span><br><span class="line"></span><br><span class="line">ToT 的优势：</span><br><span class="line">          4, 5, 6, 10</span><br><span class="line">        /      |       \</span><br><span class="line">   4+5=9    4×6=24    10-6=4</span><br><span class="line">   /    \     ✗(剩余无法凑)  /   \</span><br><span class="line"> 9×(10-6)  6×(10-5)   4×5=20  ...</span><br><span class="line"> =9×4=36✗  =6×5=30✗   20+4=24? ✗(4已用)</span><br><span class="line">                        ↑ 回溯，换路</span><br></pre></td></tr></table></figure>

<p>ToT 在每一步都可以<strong>分叉探索多个方向</strong>，在发现死路时<strong>回溯</strong>到之前的节点尝试其他路径。</p>
<h3 id="2-3-Tree-of-Thoughts（ToT）：树状搜索推理"><a href="#2-3-Tree-of-Thoughts（ToT）：树状搜索推理" class="headerlink" title="2.3 Tree-of-Thoughts（ToT）：树状搜索推理"></a>2.3 Tree-of-Thoughts（ToT）：树状搜索推理</h3><p>ToT（由 Yao 等人于 2023 年在普林斯顿提出）将推理从线性链扩展为<strong>树状结构</strong>，本质上是将经典搜索算法引入 LLM 推理过程。</p>
<p><strong>ToT 的四大核心组件：</strong></p>
<p><strong>① 思维分解（Thought Decomposition）</strong><br>将问题拆解为适当粒度的”思维单元”。粒度选择至关重要——太细则搜索空间爆炸，太粗则失去探索灵活性。比如写一篇文章，一个思维单元可以是”段落大纲”而非”单个句子”。</p>
<p><strong>② 思维生成（Thought Generation）</strong><br>在每个节点生成 k 个候选思维，有两种策略：</p>
<ul>
<li><strong>采样（Sample）</strong>：独立生成多个候选（适合创意性任务，解空间大）</li>
<li><strong>提议（Propose）</strong>：基于前文依次生成（适合逻辑性任务，避免重复）</li>
</ul>
<p><strong>③ 状态评估（State Evaluation）</strong><br>这是 ToT 最关键的创新——<strong>用 LLM 自己来评估每个中间状态的质量</strong>：</p>
<ul>
<li><strong>打分法</strong>：对每个状态评分（如 1-10 分，或”确定&#x2F;可能&#x2F;不可能”）</li>
<li><strong>投票法</strong>：让 LLM 比较多个候选，选出最有前途的</li>
</ul>
<p>评估函数充当了”导航仪”的角色，告诉搜索算法哪些方向值得继续探索、哪些应该剪枝放弃。</p>
<p><strong>④ 搜索算法（Search Algorithm）</strong><br>ToT 支持两种经典搜索策略：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BFS（广度优先）：                    DFS（深度优先）：</span><br><span class="line">层层扩展，不遗漏                     一条路走到底，走不通再回头</span><br><span class="line"></span><br><span class="line">Level 0:    [A]                     探索顺序：A → B → D → (回溯) → E → (回溯)</span><br><span class="line">Level 1:  [B] [C]                              → C → F → ✓ 找到解</span><br><span class="line">Level 2: [D][E][F][G]</span><br><span class="line"></span><br><span class="line">适合：解空间较小，需要最优解           适合：解空间大，需要尽快找到一个可行解</span><br></pre></td></tr></table></figure>

<p><strong>ToT 解决了 CoT 的三大痛点：</strong></p>
<table>
<thead>
<tr>
<th>CoT 的局限</th>
<th>ToT 的解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>不可回溯</td>
<td>DFS 自然支持回溯，发现死路可返回上一节点</td>
</tr>
<tr>
<td>无法分叉探索</td>
<td>每个节点可生成 k 个候选分支</td>
</tr>
<tr>
<td>缺乏自我评估</td>
<td>状态评估函数在每步进行质量判断</td>
</tr>
</tbody></table>
<p><strong>代价是什么？</strong> ToT 需要更多的 LLM 调用（生成 + 评估），计算成本显著高于 CoT。这是”搜索质量”与”计算成本”之间的经典权衡——和 AlphaGo 的蒙特卡洛树搜索是同一种思想。</p>
<p><img src="/images/cot-vs-tot-comparison.png" alt="CoT vs ToT 推理结构对比"><br><em>图：CoT 线性链 vs ToT 树状搜索的结构差异。CoT 只能沿单一路径前进，ToT 在每步分叉并评估，支持回溯探索。</em></p>
<h3 id="2-4-Reflexion：自我反思学习"><a href="#2-4-Reflexion：自我反思学习" class="headerlink" title="2.4 Reflexion：自我反思学习"></a>2.4 Reflexion：自我反思学习</h3><p>ToT 解决了”单次推理中的探索问题”，但还有一个更深层的问题没有解决：<strong>跨任务的经验积累</strong>。人类在失败后会反思总结教训，下次遇到类似问题时表现更好。CoT 和 ToT 都没有这种”从失败中学习”的能力——每次推理都从零开始。</p>
<p><strong>Reflexion</strong> 引入了一个关键的「反思」闭环，让 Agent 能在多次尝试之间积累经验：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Trial 1: Actor 执行 → 得到结果 → Evaluator 评判 → 失败 ❌</span><br><span class="line">         ↓</span><br><span class="line">    Self-Reflection: &quot;我在第3步选错了API，应该用search而不是lookup&quot;</span><br><span class="line">         ↓  反思摘要存入长期记忆</span><br><span class="line">Trial 2: Actor 带着反思记忆重新执行 → 改进但仍有问题 ❌</span><br><span class="line">         ↓</span><br><span class="line">    Self-Reflection: &quot;API调对了，但参数格式不对，应该用JSON而非字符串&quot;</span><br><span class="line">         ↓  追加到长期记忆</span><br><span class="line">Trial 3: Actor 综合两次教训执行 → 成功 ✅</span><br></pre></td></tr></table></figure>

<p>Reflexion 包含三个核心组件的协作循环：</p>
<ul>
<li><strong>Actor（执行器）</strong>：基于 ReAct 或 CoT 的执行引擎，负责实际操作</li>
<li><strong>Evaluator（评估器）</strong>：判断执行结果的成功&#x2F;失败，提供二元或标量反馈信号</li>
<li><strong>Self-Reflection（反思器）</strong>：最核心的创新——将失败经验转化为自然语言反思摘要，存入一个持久的<strong>语言记忆（verbal memory）</strong></li>
</ul>
<p><strong>为什么用语言记忆而不是梯度更新？</strong> 这是 Reflexion 最巧妙的设计——传统 RL 通过更新模型权重来学习，成本极高且需要大量样本。Reflexion 用自然语言存储教训（如”不要用 deprecated API v1，改用 v2”），轻量、可解释，而且在推理时通过上下文注入即可使用。</p>
<p><strong>Reflexion 在编程任务上的惊人效果：</strong></p>
<ul>
<li>HumanEval 上从 80.1%（CoT 基线）提升到 91.0%（+11%）</li>
<li>其中约 40% 的错误在第二次尝试时就被修正</li>
<li>这证明了”反思+重试”机制的强大——很多错误不需要更强的模型，只需要从失败中学到教训</li>
</ul>
<p><img src="/images/reflexion-learning-loop.png" alt="Reflexion 自我反思学习循环"><br><em>图：Reflexion 的三组件反思循环——Actor 执行、Evaluator 评判、Self-Reflection 生成语言化教训存入记忆，驱动下一次尝试改进。</em></p>
<h3 id="2-5-从-ToT-到-LATS：统一搜索、行动与反思"><a href="#2-5-从-ToT-到-LATS：统一搜索、行动与反思" class="headerlink" title="2.5 从 ToT 到 LATS：统一搜索、行动与反思"></a>2.5 从 ToT 到 LATS：统一搜索、行动与反思</h3><p>到这里，我们已经有了三种关键能力：</p>
<ul>
<li><strong>CoT&#x2F;ToT</strong>：推理时的搜索与探索</li>
<li><strong>ReAct</strong>：与外部环境的交互（工具调用）</li>
<li><strong>Reflexion</strong>：从失败中学习</li>
</ul>
<p>但这三种能力是<strong>各自独立</strong>的。ToT 只做推理搜索，不调用工具；ReAct 调用工具但不做搜索；Reflexion 做反思但搜索策略很原始。有没有一个框架能把这三者统一起来？</p>
<p><strong>LATS（Language Agent Tree Search）</strong> 正是这个统一框架。它将蒙特卡洛树搜索（MCTS）——AlphaGo 的核心算法——引入 LLM Agent 决策，将搜索、行动和反思融合为一个整体：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">                   ┌──────────────────────┐</span><br><span class="line">                   │    MCTS 搜索循环      │</span><br><span class="line">                   └──────────────────────┘</span><br><span class="line">                             │</span><br><span class="line">   ┌─────────────┬──────────┼──────────┬──────────────┐</span><br><span class="line">   ▼             ▼          ▼          ▼              ▼</span><br><span class="line">Selection    Expansion  Simulation  Evaluation   Backpropagation</span><br><span class="line">(选择节点)   (生成动作)  (执行动作)  (LLM评分)    (反向传播)</span><br><span class="line">   │             │          │          │              │</span><br><span class="line"> UCB1算法    LLM生成     调用工具    价值评估      更新节点</span><br><span class="line"> 平衡探索     候选动作    获取反馈    + 反思        质量分数</span><br><span class="line"> 与利用                              </span><br></pre></td></tr></table></figure>

<p><strong>MCTS 五步循环详解：</strong></p>
<p><strong>① Selection（选择）</strong>：用 UCB1 公式选择最值得探索的节点，自动平衡”深入已知好路径”和”尝试未探索方向”——这解决了 ToT 简单 BFS&#x2F;DFS 策略的效率问题。</p>
<p><strong>② Expansion（扩展）</strong>：在选中节点用 LLM 生成 n 个候选动作，每个动作可以是推理步骤或工具调用——这融合了 ToT 的分支能力和 ReAct 的工具交互。</p>
<p><strong>③ Simulation（模拟）</strong>：执行动作并观察环境反馈——ReAct 的核心循环。</p>
<p><strong>④ Evaluation（评估）</strong>：LLM 对当前状态进行价值评估，给出分数。<strong>关键创新：如果检测到失败，触发 Reflexion 式的自我反思，生成反思摘要指导后续搜索。</strong></p>
<p><strong>⑤ Backpropagation（反向传播）</strong>：将评估分数沿树路径回传，更新每个祖先节点的质量估计——这让 LATS 能从全局视角优化搜索方向。</p>
<p><strong>LATS &#x3D; ToT（搜索框架）+ ReAct（环境交互）+ Reflexion（失败学习）</strong></p>
<p>这种统一带来的效果是显著的：在 HotPotQA 多跳推理任务上，LATS 比单独的 ReAct 提升 16%，比 Reflexion 提升 8%，比 ToT 提升 12%。代价是更高的计算成本——但这正是”用推理时计算换取更好决策”的核心思想。</p>
<p><img src="/images/lats-mcts-agent.png" alt="LATS 蒙特卡洛树搜索 Agent 决策"><br><em>图：LATS 将 MCTS 的五步循环应用于 Agent 决策，统一了搜索（ToT）、行动（ReAct）和反思（Reflexion）三大能力。</em></p>
<hr>
<h2 id="三、Deep-Agent-架构：当任务跨越百步"><a href="#三、Deep-Agent-架构：当任务跨越百步" class="headerlink" title="三、Deep Agent 架构：当任务跨越百步"></a>三、Deep Agent 架构：当任务跨越百步</h2><p>LATS 统一了搜索、行动和反思，但它仍然在<strong>单个上下文窗口</strong>内运作。当任务复杂度从”几步完成”升级到”数十甚至上百步”——比如写一份完整的研究报告、重构一个大型代码库、或执行一个跨越数小时的深度研究——上下文窗口就成了不可逾越的瓶颈：推理历史、工具返回、中间结果……全部挤在有限的 token 窗口中，信噪比急剧下降。</p>
<p><strong>Deep Agent（深度智能体）</strong> 架构是 2025 年下半年兴起的新范式，代表产品包括 Claude Code、OpenAI Deep Research、Manus AI 等。它的核心思想是：**将 Agent 的认知从”上下文内”扩展到”上下文外”**——用外部持久化系统弥补上下文窗口的局限。</p>
<p><img src="/images/agent-architecture-evolution.png" alt="Agent 架构演进全景图"><br><em>图：从 ReAct 到 Deep Agent 的架构演进。每一步进化都在解决前一代的核心瓶颈：ReAct 解决了工具交互、CoT&#x2F;ToT 解决了推理搜索、Reflexion 解决了经验学习、Deep Agent 解决了长任务上下文管理。</em></p>
<h3 id="3-1-四大支柱"><a href="#3-1-四大支柱" class="headerlink" title="3.1 四大支柱"></a>3.1 四大支柱</h3><p>Deep Agent 架构建立在四个基础之上：</p>
<p><strong>① 显式规划（Explicit Planning）</strong><br>不依赖 LLM 隐式推理，而是维护一个<strong>外部的、可持久化的任务计划</strong>。计划可以被检查、修改和恢复。这意味着即使上下文窗口被清空，Agent 仍然知道自己在做什么、做到了哪一步。</p>
<p>以 Claude Code 为例：当它重构一个大型代码库时，会在文件系统中写入一份 <code>plan.md</code>，记录每个模块的改造状态。即使中间因为上下文溢出导致会话重置，Agent 读取 plan.md 后就能无缝继续。</p>
<p><strong>② 层级委派（Hierarchical Delegation）</strong><br>单个 Agent 的能力总有上限。Deep Agent 将复杂任务拆分给<strong>专门化的子 Agent</strong>，每个子 Agent 有独立的上下文窗口和专属工具集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Orchestrator Agent（指挥官）</span><br><span class="line">├── Research Sub-Agent（负责信息检索）→ 有搜索工具</span><br><span class="line">├── Code Sub-Agent（负责代码编写）→ 有文件读写和终端</span><br><span class="line">├── Review Sub-Agent（负责质量审查）→ 有测试工具</span><br><span class="line">└── Memory Sub-Agent（负责信息管理）→ 有知识库</span><br></pre></td></tr></table></figure>

<p>这种设计的关键优势是<strong>上下文隔离</strong>：Research Agent 的搜索结果不会污染 Code Agent 的编码上下文。每个子 Agent 只接收与自己任务相关的信息，信噪比大幅提升。</p>
<p><strong>③ 持久化记忆（Persistent Memory）</strong><br>使用文件系统作为外部记忆，而非仅依赖上下文窗口。Agent 可以：</p>
<ul>
<li>写入结构化笔记和发现摘要</li>
<li>读取之前的研究结果</li>
<li>维护状态文件跟踪进度</li>
<li>建立知识索引便于快速检索</li>
</ul>
<p>这本质上是将人类研究员的”做笔记”习惯编码为 Agent 的核心行为——上下文窗口是”工作记忆”（短期），文件系统是”笔记本”（长期）。</p>
<p><strong>④ 极致的上下文工程（Extreme Context Engineering）</strong><br>精心管理什么信息进入上下文窗口。具体技术包括：</p>
<ul>
<li><strong>渐进式摘要</strong>：每隔 N 步将历史压缩为摘要</li>
<li><strong>选择性加载</strong>：只加载与当前子任务相关的信息</li>
<li><strong>上下文分层</strong>：系统提示 &gt; 当前任务 &gt; 相关历史 &gt; 可选参考</li>
<li><strong>智能截断</strong>：工具返回过长时自动截取关键部分</li>
</ul>
<p>以 OpenClaw 为例——它在每次 heartbeat 时读取 HEARTBEAT.md（而非全部历史），在每个 session 开始时读取 SOUL.md 和 USER.md（身份信息），只有在主 session 中才加载 MEMORY.md（长期记忆），这就是上下文工程的实际应用。</p>
<h3 id="3-2-CORAL：认知资源自分配"><a href="#3-2-CORAL：认知资源自分配" class="headerlink" title="3.2 CORAL：认知资源自分配"></a>3.2 CORAL：认知资源自分配</h3><p><strong>CORAL（Cognitive Resource Self-Allocation）</strong> 是 ICLR 2026 收录的工作，专门解决长任务中 Agent 的「注意力漂移」问题——当上下文中积累了太多无关信息，LLM 的注意力被分散，推理质量急剧下降。</p>
<p><strong>核心洞察：</strong> 人类处理长任务时，会主动”清空短期记忆”——比如写论文写了3小时后，会先休息，回来后重新读一遍大纲，而不是试图记住之前的每一个细节。CORAL 给 Agent 提供了类似的能力。</p>
<p><strong>工作记忆管理工具集：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Agent 工具箱中新增三种&quot;元工具&quot;：</span><br><span class="line">1. set_checkpoint(label)  → 在关键节点保存状态快照</span><br><span class="line">2. clear_memory()         → 清除工作记忆中的杂乱信息  </span><br><span class="line">3. restore(label)         → 从指定检查点恢复推理上下文</span><br></pre></td></tr></table></figure>

<p>当 Agent 探索了多条路径、积累了大量搜索结果，发现自己”迷失方向”时，可以主动调用 <code>clear_memory()</code> + <code>restore(&quot;initial_plan&quot;)</code> 来重新开始——但不是完全从零开始，而是保留了检查点中的关键发现。</p>
<p><strong>训练方式：</strong> CORAL 使用<strong>多轮 Agentic 强化策略优化（Multi-episode Agentic Reinforced Policy Optimization）</strong> 算法，让 Agent 通过 RL 学会三个关键判断：</p>
<ul>
<li><strong>何时设置检查点</strong>（在做出重要发现或关键决策后）</li>
<li><strong>何时清理记忆</strong>（当上下文信噪比过低时）</li>
<li><strong>恢复到哪个检查点</strong>（选择最有价值的历史状态）</li>
</ul>
<p>CORAL 在 SWE-bench 等长任务基准上显著优于没有记忆管理的 Agent，验证了”主动管理认知资源”的价值——这也为 Deep Agent 架构的”极致上下文工程”提供了理论支撑。</p>
<hr>
<h2 id="四、Agentic-RL：从工程拼接到端到端学习"><a href="#四、Agentic-RL：从工程拼接到端到端学习" class="headerlink" title="四、Agentic RL：从工程拼接到端到端学习"></a>四、Agentic RL：从工程拼接到端到端学习</h2><p>前三节的所有架构——从 ReAct 到 LATS 到 Deep Agent——都属于「推理时（inference-time）」的工程技巧。它们通过精巧的提示设计、搜索算法和记忆管理来提升 Agent 表现，但<strong>模型本身并没有因此变得更强</strong>。模型权重是冻结的，所有的”聪明”都来自外部框架。</p>
<p>这就像给一个普通人配备了最好的工具箱、最详细的操作手册——他确实能完成更复杂的任务，但他自身的能力并没有提升。如果工具箱被拿走或手册不适用，他就回到了原点。</p>
<p><strong>Agentic RL</strong> 代表了一个根本性的范式转移：<strong>直接通过强化学习训练 LLM 的 Agent 行为能力</strong>，让模型在多步交互中学会规划、工具使用、错误修正——这些能力被编码进模型权重，而非依赖外部框架。</p>
<h3 id="4-1-从-RLHF-到-Agentic-RL"><a href="#4-1-从-RLHF-到-Agentic-RL" class="headerlink" title="4.1 从 RLHF 到 Agentic RL"></a>4.1 从 RLHF 到 Agentic RL</h3><table>
<thead>
<tr>
<th>维度</th>
<th>RLHF</th>
<th>Agentic RL</th>
</tr>
</thead>
<tbody><tr>
<td>目标</td>
<td>让 LLM 输出更符合人类偏好</td>
<td>让 LLM 学会多步决策与工具使用</td>
</tr>
<tr>
<td>交互</td>
<td>单轮：prompt → response</td>
<td>多轮：action → env feedback → action</td>
</tr>
<tr>
<td>奖励</td>
<td>人类偏好评分</td>
<td>任务完成度 + 过程奖励</td>
</tr>
<tr>
<td>训练格式</td>
<td>单条序列</td>
<td>多轮轨迹（trajectory）</td>
</tr>
<tr>
<td>环境</td>
<td>无</td>
<td>真实或模拟环境</td>
</tr>
</tbody></table>
<p>Agentic RL 的核心突破在于：<strong>将 LLM 从被动的序列生成器重新定义为主动的、嵌入复杂动态世界的决策智能体。</strong></p>
<p><img src="/images/agentic-rl-paradigm.png" alt="Agentic RL 范式转移"><br><em>图：从推理时工程（左）到 Agentic RL 端到端训练（右）的范式转移——外部脚手架 vs 内化能力。</em></p>
<h3 id="4-2-Agent-R1：端到端-Agent-强化学习"><a href="#4-2-Agent-R1：端到端-Agent-强化学习" class="headerlink" title="4.2 Agent-R1：端到端 Agent 强化学习"></a>4.2 Agent-R1：端到端 Agent 强化学习</h3><p><strong>Agent-R1</strong>（中国科学技术大学，2025.11）是将 DeepSeek-R1 的 RL 训练范式扩展到 Agent 场景的里程碑工作。</p>
<p><strong>核心问题：为什么不能直接把 RLHF&#x2F;GRPO 套到 Agent 上？</strong></p>
<p>传统 RL 训练 LLM 时，轨迹（trajectory）是一个单轮序列：prompt → response。但 Agent 的轨迹是多轮交互序列，包含两种本质不同的 token：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">轨迹结构：</span><br><span class="line">[System Prompt] → Agent生成思考 → Agent调用工具 → 环境返回结果 → Agent继续思考 → ...</span><br><span class="line">                  ↑ Agent token（可训练）      ↑ 环境 token（不可训练！）</span><br></pre></td></tr></table></figure>

<p>关键区分：<strong>Agent 生成的 token 需要参与梯度计算，但环境返回的 token 不应该</strong>——因为你不能通过训练模型来改变环境的行为。Agent-R1 在 MDP 框架中明确建模了这个区分。</p>
<p><strong>MDP 扩展详解：</strong></p>
<table>
<thead>
<tr>
<th>组件</th>
<th>静态 LLM</th>
<th>Agent-R1</th>
</tr>
</thead>
<tbody><tr>
<td><strong>状态空间</strong></td>
<td>prompt + 已生成 token</td>
<td>完整对话历史 + 每轮环境反馈</td>
</tr>
<tr>
<td><strong>动作空间</strong></td>
<td>词表中选下一个 token</td>
<td>同上，但 token 序列可触发工具调用</td>
</tr>
<tr>
<td><strong>转移函数</strong></td>
<td>确定性（拼接 token）</td>
<td><strong>随机性</strong>（环境返回不确定）</td>
</tr>
<tr>
<td><strong>奖励函数</strong></td>
<td>单次终端奖励</td>
<td>终端奖励 + 中间过程奖励</td>
</tr>
</tbody></table>
<p><strong>过程奖励（Process Rewards）</strong> 是 Agent-R1 的重要创新——不只在任务完成时给奖励，在中间步骤也给信号。比如：正确调用了 search API 但查询词不够精确，可以给一个小的正奖励（鼓励工具使用）但不是满分（查询还需优化）。这解决了长任务中”奖励稀疏”的经典难题。</p>
<p>Agent-R1 开源了完整的训练框架（基于 veRL），支持快速接入不同环境，已在 Multi-hop QA 上验证了效果。</p>
<h3 id="4-3-AgentRL：多任务多轮-Agent-训练框架"><a href="#4-3-AgentRL：多任务多轮-Agent-训练框架" class="headerlink" title="4.3 AgentRL：多任务多轮 Agent 训练框架"></a>4.3 AgentRL：多任务多轮 Agent 训练框架</h3><p><strong>AgentRL</strong>（清华大学 THUDM，2025.10）是目前最系统的 Agentic RL 训练框架，其训练成果已应用于智谱的 AutoGLM。</p>
<p><strong>两大技术创新：</strong></p>
<p><strong>① 跨策略采样（Cross-Policy Sampling）</strong><br>在多轮设定中，Agent 容易陷入策略过拟合，不愿探索新策略。AgentRL 通过从多个模型策略池中采样动作，增强探索多样性：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">轨迹生成时：</span><br><span class="line">Step 1: 从 Policy_A 采样 action</span><br><span class="line">Step 2: 从 Policy_B 采样 action  ← 跨策略</span><br><span class="line">Step 3: 从 Policy_A 采样 action</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><strong>② 任务优势归一化（Task Advantage Normalization）</strong><br>多任务训练时不同任务的奖励尺度差异大。对每个任务的优势值独立归一化，稳定训练：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Advantage_normalized = (Advantage - mean_task) / std_task</span><br></pre></td></tr></table></figure>

<p><strong>实验结果惊人：</strong> AgentRL 在五个 Agent 基准任务（ALFWorld、DB、KG、OS、Webshop）上训练开源 LLM（Qwen2.5），性能显著超越 GPT-5、Claude-Sonnet-4 和 DeepSeek-R1。</p>
<h3 id="4-4-DeepResearcher：真实环境中的-RL-训练"><a href="#4-4-DeepResearcher：真实环境中的-RL-训练" class="headerlink" title="4.4 DeepResearcher：真实环境中的 RL 训练"></a>4.4 DeepResearcher：真实环境中的 RL 训练</h3><p><strong>DeepResearcher</strong>（上海交通大学 GAIR，2025.4，EMNLP 2025 收录）是首个在真实 Web 搜索环境中端到端训练 Agent 的框架。</p>
<p><strong>为什么 RAG 环境训练不够？</strong></p>
<p>之前的 RL 训练工作（如 Search-R1、R1-Searcher）都在 RAG 环境中进行——给模型一个固定语料库，模型从中检索信息。这种方式有一个致命假设：<strong>所有需要的信息已经在语料库里了</strong>。但现实世界不是这样的：</p>
<ul>
<li>信息可能不存在于语料库中</li>
<li>信息可能已经过时</li>
<li>需要跨多个领域综合多个来源</li>
<li>网页格式杂乱，充满噪声和反爬机制</li>
</ul>
<p>DeepResearcher 直接在开放互联网环境中训练，Agent 需要面对真实的搜索引擎、真实的网页（包括乱码、广告、反爬）。</p>
<p><strong>多 Agent 架构设计：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Main Agent（推理决策者）</span><br><span class="line">    │</span><br><span class="line">    ├── 决定搜索什么关键词</span><br><span class="line">    ├── 分析搜索结果摘要</span><br><span class="line">    ├── 决定深入哪些网页</span><br><span class="line">    │       ↓</span><br><span class="line">    └── Browsing Agent（网页浏览者）</span><br><span class="line">            ├── 加载完整网页</span><br><span class="line">            ├── 提取相关信息</span><br><span class="line">            └── 返回结构化摘要给 Main Agent</span><br></pre></td></tr></table></figure>

<p>Main Agent 负责高层决策（搜什么、看哪个、如何综合），Browsing Agent 负责底层信息提取——这比 RAG 系统”直接返回文本片段”要灵活得多。</p>
<p><strong>训练后涌现的四种认知行为（最惊喜的发现）：</strong></p>
<ol>
<li><p><strong>自主规划（Planning）</strong>：Agent 自动学会了在开始研究前制定计划，并在过程中动态调整。注意——<strong>没有人教它规划</strong>，这是纯 RL 训练涌现的行为！甚至还会”合并步骤”来提高效率。</p>
</li>
<li><p><strong>交叉验证（Cross-Validation）</strong>：Agent 找到一个答案后，不会立即接受，而是继续搜索其他来源来验证。这种”不轻信第一个结果”的审慎行为也是自发涌现的。</p>
</li>
<li><p><strong>自我反思与重定向（Self-Reflection）</strong>：发现当前搜索方向不对时，Agent 会主动调整关键词或换一个完全不同的搜索策略。</p>
</li>
<li><p><strong>诚实性（Honesty）</strong>：当确实找不到确定答案时，Agent 会坦诚说明，而非编造一个看似合理的答案。</p>
</li>
</ol>
<p><strong>量化结果：</strong> DeepResearcher 在 7 个开放域研究数据集上，比提示工程方案提升高达 <strong>28.9 分</strong>，比 RAG 环境 RL 方案提升 <strong>7.2 分</strong>。这证明了一个核心结论：<strong>在真实环境中训练不是可选的优化，而是开发稳健研究能力的根本需求。</strong></p>
<h3 id="4-5-通义-DeepResearch：全栈-Agent-训练流水线"><a href="#4-5-通义-DeepResearch：全栈-Agent-训练流水线" class="headerlink" title="4.5 通义 DeepResearch：全栈 Agent 训练流水线"></a>4.5 通义 DeepResearch：全栈 Agent 训练流水线</h3><p>阿里通义团队（2025.9）提出了一套完整的 Agent 训练流程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">阶段1: Agentic Pre-training（预训练阶段引入工具使用能力）</span><br><span class="line">    ↓</span><br><span class="line">阶段2: Supervised Fine-tuning（用专家数据冷启动）</span><br><span class="line">    ↓</span><br><span class="line">阶段3: On-policy RL（在线强化学习自进化）</span><br></pre></td></tr></table></figure>

<p>这套「预训练 → SFT → RL」的三阶段流程被验证为训练 Deep Research Agent 的有效范式。</p>
<hr>
<h2 id="五、推理时搜索：第三条路径"><a href="#五、推理时搜索：第三条路径" class="headerlink" title="五、推理时搜索：第三条路径"></a>五、推理时搜索：第三条路径</h2><p>前四节讨论了两条提升 Agent 能力的路径：</p>
<ul>
<li><strong>路径 A：推理时工程</strong>（更好的框架、搜索算法、记忆管理）</li>
<li><strong>路径 B：训练时学习</strong>（通过 RL 直接提升模型能力）</li>
</ul>
<p>还有<strong>路径 C</strong>——<strong>在推理时投入更多计算</strong>。不改变模型权重，也不依赖复杂的外部框架，而是让模型”想更久”。</p>
<h3 id="5-1-推理时计算扩展（Inference-Time-Scaling）"><a href="#5-1-推理时计算扩展（Inference-Time-Scaling）" class="headerlink" title="5.1 推理时计算扩展（Inference-Time Scaling）"></a>5.1 推理时计算扩展（Inference-Time Scaling）</h3><p>OpenAI 的 o1&#x2F;o3&#x2F;o4 系列揭示了一个令人振奋的发现：<strong>推理时的计算量和推理质量之间存在近似对数线性的正相关关系</strong>。换句话说，让模型多花 10 倍算力”思考”，可以获得显著的质量提升。</p>
<p>核心技术手段包括：</p>
<ul>
<li><strong>内部思维链（Internal CoT）</strong>：模型在输出前进行长链隐式推理，这些推理 token 消耗计算但不一定展示给用户</li>
<li><strong>搜索与回溯</strong>：多条推理路径并行探索，选择最优路径——本质上和 ToT 异曲同工，但被编码进了模型的推理行为中</li>
<li><strong>自我验证</strong>：模型生成候选答案后，自己检查答案的正确性，如有问题则重新推理</li>
<li><strong>自适应计算分配</strong>：简单问题少想，复杂问题多想——模型学会了”量力而行”</li>
</ul>
<p>这解释了为什么 o3&#x2F;o4 在某些数学竞赛题上表现惊人——它们可能在单个问题上花费了相当于普通模型数百次调用的算力。</p>
<h3 id="5-2-AB-MCTS：自适应分支蒙特卡洛树搜索"><a href="#5-2-AB-MCTS：自适应分支蒙特卡洛树搜索" class="headerlink" title="5.2 AB-MCTS：自适应分支蒙特卡洛树搜索"></a>5.2 AB-MCTS：自适应分支蒙特卡洛树搜索</h3><p><strong>AB-MCTS（Adaptive Branching MCTS）</strong>（Sakana AI）将这个思想推向了多模型协作的维度：</p>
<p><strong>核心思想：</strong> 不是一个模型自己搜索，而是<strong>多个不同的 LLM 协作进行蒙特卡洛树搜索</strong>。每个模型有不同的偏好和盲点，多模型搜索可以获得更全面的探索覆盖。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">搜索树的每个节点：</span><br><span class="line">├── GPT-5 生成候选 A → 评分 0.7</span><br><span class="line">├── Claude 生成候选 B → 评分 0.9  ← 选中展开</span><br><span class="line">└── Gemini 生成候选 C → 评分 0.5</span><br><span class="line"></span><br><span class="line">下一层继续多模型扩展...</span><br></pre></td></tr></table></figure>

<p><strong>自适应分支</strong> 是关键创新：不固定每个节点的分支数，而是根据当前问题的难度和搜索进展动态调整。简单部分少分支快速通过，困难部分多分支深度探索。</p>
<p>AB-MCTS 代表了推理时搜索的前沿方向：不改变任何模型的权重，而是通过更聪明的搜索编排来突破单模型的能力上限。这和下棋中的思想完全一致——棋手的水平（模型能力）是固定的，但花更多时间思考（搜索更多变化）总能下出更好的棋。</p>
<h3 id="5-3-三条路径的关系"><a href="#5-3-三条路径的关系" class="headerlink" title="5.3 三条路径的关系"></a>5.3 三条路径的关系</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">         Agent 能力提升</span><br><span class="line">        /       |       \</span><br><span class="line">  路径 A        路径 B        路径 C</span><br><span class="line">推理时工程    训练时学习    推理时计算</span><br><span class="line">(ReAct,ToT,   (Agentic RL,  (o1-style,</span><br><span class="line"> Deep Agent)   Agent-R1)     AB-MCTS)</span><br><span class="line">     ↓            ↓            ↓</span><br><span class="line">模型不变,       模型变强,     模型不变,</span><br><span class="line">外部框架优化    内化能力      多花算力思考</span><br></pre></td></tr></table></figure>

<p>实践中，三条路径并不互斥——最强的 Agent 系统同时使用了全部三条：强 RL 训练的基座模型（路径 B）+ 推理时多步搜索（路径 C）+ 外部记忆和工具管理（路径 A）。</p>
<hr>
<h2 id="六、实用建议：如何选择-Agent-架构"><a href="#六、实用建议：如何选择-Agent-架构" class="headerlink" title="六、实用建议：如何选择 Agent 架构"></a>六、实用建议：如何选择 Agent 架构</h2><h3 id="任务复杂度-vs-架构选择"><a href="#任务复杂度-vs-架构选择" class="headerlink" title="任务复杂度 vs 架构选择"></a>任务复杂度 vs 架构选择</h3><table>
<thead>
<tr>
<th>任务类型</th>
<th>推荐架构</th>
<th>代表方案</th>
</tr>
</thead>
<tbody><tr>
<td>简单工具调用（天气&#x2F;搜索）</td>
<td>ReAct</td>
<td>LangChain ReAct Agent</td>
</tr>
<tr>
<td>多步骤有序任务</td>
<td>Plan-and-Execute</td>
<td>LangGraph</td>
</tr>
<tr>
<td>需要探索的复杂推理</td>
<td>Tree-of-Thoughts &#x2F; LATS</td>
<td>自定义</td>
</tr>
<tr>
<td>需要从失败中学习</td>
<td>Reflexion</td>
<td>自定义</td>
</tr>
<tr>
<td>超长任务（100+ 步）</td>
<td>Deep Agent</td>
<td>Claude Code &#x2F; OpenClaw</td>
</tr>
<tr>
<td>训练专用 Agent</td>
<td>Agentic RL</td>
<td>AgentRL &#x2F; Agent-R1</td>
</tr>
<tr>
<td>深度研究</td>
<td>Deep Agent + RL</td>
<td>DeepResearcher</td>
</tr>
</tbody></table>
<h3 id="关键设计原则"><a href="#关键设计原则" class="headerlink" title="关键设计原则"></a>关键设计原则</h3><ol>
<li><strong>外部化一切状态</strong>：不要仅依赖上下文窗口，用文件系统持久化记忆</li>
<li><strong>分层委派</strong>：复杂任务拆分给专门的 Sub-Agent</li>
<li><strong>显式管理上下文</strong>：主动摘要和压缩，保持高信噪比</li>
<li><strong>建立检查点机制</strong>：允许 Agent 回溯和恢复</li>
<li><strong>过程奖励优于结果奖励</strong>：在训练中引入中间步骤的奖励信号</li>
</ol>
<hr>
<h2 id="七、展望：Agent-技术的下一步"><a href="#七、展望：Agent-技术的下一步" class="headerlink" title="七、展望：Agent 技术的下一步"></a>七、展望：Agent 技术的下一步</h2><h3 id="2026-年关键趋势"><a href="#2026-年关键趋势" class="headerlink" title="2026 年关键趋势"></a>2026 年关键趋势</h3><ol>
<li><strong>Agentic RL 成为标配</strong>：从提示工程走向端到端训练，直接优化 Agent 的多步决策能力</li>
<li><strong>Memory 成为一等公民</strong>：ICLR 2026 专门设立 MemAgents Workshop，记忆管理从「工程技巧」升级为核心研究方向</li>
<li><strong>多模态 Agent</strong>：Agent 不再限于文本交互，可以「看」屏幕、操作 UI、理解视觉信息</li>
<li><strong>自进化 Agent</strong>：Agent 能在部署后持续从真实交互中学习改进</li>
</ol>
<h3 id="核心挑战"><a href="#核心挑战" class="headerlink" title="核心挑战"></a>核心挑战</h3><ul>
<li><strong>安全与对齐</strong>：自主度越高，风险越大，如何确保 Agent 行为安全可控</li>
<li><strong>长期记忆</strong>：如何在超长任务中维持一致的目标和上下文</li>
<li><strong>奖励设计</strong>：复杂任务的奖励信号如何定义和分解</li>
<li><strong>评估基准</strong>：缺乏真正长时间跨度的 Agent 评估标准</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>Yao, S. et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ICLR 2023.</li>
<li>Shinn, N. et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.” NeurIPS 2023.</li>
<li>Wei, J. et al. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” NeurIPS 2022.</li>
<li>Yao, S. et al. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” NeurIPS 2023.</li>
<li>Zhou, A. et al. “Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models.” ICML 2024.</li>
<li>Cheng, M. et al. “Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning.” arXiv:2511.14460, Nov 2025.</li>
<li>Zhang, H. et al. “AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework.” arXiv:2510.04206, Oct 2025.</li>
<li>Zheng, Y. et al. “DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments.” arXiv:2504.03160, Apr 2025.</li>
<li>Wang, R. et al. “A Practitioner’s Guide to Multi-turn Agentic Reinforcement Learning.” arXiv:2510.01132, Oct 2025.</li>
<li>“Don’t Lose the Thread: Empowering Long-Horizon LLM Agents with Cognitive Resource Self-Allocation (CORAL).” ICLR 2026.</li>
<li>“The Landscape of Agentic Reinforcement Learning for LLMs: A Survey.” TMLR, Jan 2026.</li>
<li>Tongyi Team. “Tongyi DeepResearch: A New Era of Open-Source AI Researchers.” Sep 2025.</li>
<li>OpenAI. “Introducing Deep Research.” Feb-Jul 2025.</li>
<li>Sakana AI. “Inference-Time Scaling and Collective Intelligence for Frontier AI (AB-MCTS).” 2025.</li>
</ol>
<hr>
<blockquote>
<p>本文系统梳理了 Agent Loop 和 Agent RL 领域的核心算法和最新进展，从经典的 ReAct 循环到前沿的 Agentic RL 训练范式。Agent 技术正在从「工程拼接」走向「端到端学习」，这将是 2026 年 AI 领域最重要的技术方向之一。</p>
</blockquote>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/headshot.jpeg" alt="内森 Nathan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">内森 Nathan</p><p class="is-size-6 is-block">Senior Software Engineer Manager @ Microsoft</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">44</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">110</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hydraxman" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hydraxman"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/AI-%E6%8A%80%E6%9C%AF%E6%B7%B1%E5%BA%A6/"><span class="level-start"><span class="level-item">AI 技术深度</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%88%A8%E7%A0%81%E9%97%AE%E5%BA%95/"><span class="level-start"><span class="level-item">刨码问底</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AE%9E%E6%88%98/"><span class="level-start"><span class="level-item">实战</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%89%93%E7%A0%81%E8%A6%81%E4%B9%89/"><span class="level-start"><span class="level-item">打码要义</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF/"><span class="level-start"><span class="level-item">技术</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90/"><span class="level-start"><span class="level-item">技术分析</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/"><span class="level-start"><span class="level-item">技术教程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E6%B7%B1%E5%BA%A6/"><span class="level-start"><span class="level-item">技术深度</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"><span class="level-start"><span class="level-item">技术调研</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90/"><span class="level-start"><span class="level-item">深度分析</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E6%8A%A5%E9%81%93/"><span class="level-start"><span class="level-item">深度报道</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%A7%91%E6%8A%80/"><span class="level-start"><span class="level-item">科技</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E7%A7%91%E6%8A%80/AI%E4%B8%8E%E6%9C%BA%E5%99%A8%E4%BA%BA/"><span class="level-start"><span class="level-item">AI与机器人</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%A7%91%E6%8A%80%E8%B5%84%E8%AE%AF/"><span class="level-start"><span class="level-item">科技资讯</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%A7%91%E6%8A%80%E9%80%9F%E9%80%92/"><span class="level-start"><span class="level-item">科技速递</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">英语学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-16T16:00:00.000Z">2026-02-17</time></p><p class="title"><a href="/2026/02/17/foreign-reactions-spring-gala-robot-2025/">从秧BOT到武BOT：宇树机器人用醉拳和双截棍炸翻2026春晚，全球再次沸腾</a></p><p class="categories"><a href="/categories/%E7%A7%91%E6%8A%80/">科技</a> / <a href="/categories/%E7%A7%91%E6%8A%80/AI%E4%B8%8E%E6%9C%BA%E5%99%A8%E4%BA%BA/">AI与机器人</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-16T16:00:00.000Z">2026-02-17</time></p><p class="title"><a href="/2026/02/17/english-learning-spring-gala-robot-reactions/">机器人打醉拳！从老外评价春晚《武BOT》学15个地道英语表达</a></p><p class="categories"><a href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/">英语学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-16T02:00:00.000Z">2026-02-16</time></p><p class="title"><a href="/2026/02/16/2026-02-16-morning-news/">2026年02月16日早间要闻</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-15T16:00:00.000Z">2026-02-16</time></p><p class="title"><a href="/2026/02/16/openclaw-vs-zeroclaw/">ZeroClaw 横空出世，它会取代 OpenClaw 吗？</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90/">技术分析</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-15T16:00:00.000Z">2026-02-16</time></p><p class="title"><a href="/2026/02/16/openclaw-founder-joins-openai/">OpenClaw 创始人加入 OpenAI：Agent 工程进入新阶段？</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90/">技术分析</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2026/02/"><span class="level-start"><span class="level-item">二月 2026</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">十一月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">十月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/03/"><span class="level-start"><span class="level-item">三月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">二月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/15%E5%88%86%E9%92%9F/"><span class="tag">15分钟</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/2026/"><span class="tag">2026</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/2026-Trends/"><span class="tag">2026 Trends</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">28</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI-Agent/"><span class="tag">AI Agent</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AOP/"><span class="tag">AOP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AST/"><span class="tag">AST</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Agent/"><span class="tag">Agent</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Agentic-Engineering/"><span class="tag">Agentic Engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Android/"><span class="tag">Android</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Claude/"><span class="tag">Claude</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Code-design/"><span class="tag">Code design</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Codex/"><span class="tag">Codex</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Copilot/"><span class="tag">Copilot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cursor/"><span class="tag">Cursor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Debug/"><span class="tag">Debug</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepSeek/"><span class="tag">DeepSeek</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Design-Patterns/"><span class="tag">Design Patterns</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DevOps/"><span class="tag">DevOps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/English-Learning/"><span class="tag">English-Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GLM-5/"><span class="tag">GLM-5</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GUI-Agent/"><span class="tag">GUI Agent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GitHub/"><span class="tag">GitHub</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GoF/"><span class="tag">GoF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradle/"><span class="tag">Gradle</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hacker-News/"><span class="tag">Hacker News</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Industry-Thoughts/"><span class="tag">Industry Thoughts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JVM/"><span class="tag">JVM</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSP/"><span class="tag">LSP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lovart/"><span class="tag">Lovart</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MCP/"><span class="tag">MCP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenAI/"><span class="tag">OpenAI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenClaw/"><span class="tag">OpenClaw</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimus/"><span class="tag">Optimus</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Prompt-Engineering/"><span class="tag">Prompt Engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/React/"><span class="tag">React</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reading/"><span class="tag">Reading</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Robotics/"><span class="tag">Robotics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Rust/"><span class="tag">Rust</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Science/"><span class="tag">Science</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Seedance/"><span class="tag">Seedance</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SpaceX/"><span class="tag">SpaceX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spring-Festival-Gala/"><span class="tag">Spring-Festival-Gala</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Technology/"><span class="tag">Technology</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Trending/"><span class="tag">Trending</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UI%E8%AE%BE%E8%AE%A1/"><span class="tag">UI设计</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UML/"><span class="tag">UML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unitree/"><span class="tag">Unitree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows/"><span class="tag">Windows</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ZeroClaw/"><span class="tag">ZeroClaw</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bytecode/"><span class="tag">bytecode</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/macOS/"><span class="tag">macOS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%AD%E5%9B%BD%E5%88%B6%E9%80%A0/"><span class="tag">中国制造</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><span class="tag">人工智能</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E5%BD%A2%E6%9C%BA%E5%99%A8%E4%BA%BA/"><span class="tag">人形机器人</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"><span class="tag">代码分析</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BB%A3%E7%A0%81%E5%8F%AF%E8%A7%86%E5%8C%96/"><span class="tag">代码可视化</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%9B%E6%96%B0/"><span class="tag">创新</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8E%86%E5%8F%B2/"><span class="tag">历史</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%95%86%E4%B8%9A/"><span class="tag">商业</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BD%E5%86%85/"><span class="tag">国内</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%8F%E4%BB%A4%E6%97%B6/"><span class="tag">夏令时</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">多模态大模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">大模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%AA%E7%A9%BA%E8%AE%A1%E7%AE%97/"><span class="tag">太空计算</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"><span class="tag">字节码</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"><span class="tag">字节跳动</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AE%87%E6%A0%91%E7%A7%91%E6%8A%80/"><span class="tag">宇树科技</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AE%89%E5%85%A8/"><span class="tag">安全</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AF%B9%E6%AF%94%E8%AF%84%E6%B5%8B/"><span class="tag">对比评测</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="tag">开发工具</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E6%BA%90/"><span class="tag">开源</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="tag">强化学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%BD%AF/"><span class="tag">微软</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%80%E6%9C%AF%E6%97%A5%E6%8A%A5/"><span class="tag">技术日报</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF/"><span class="tag">技术趋势</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8F%90%E7%A4%BA%E8%AF%8D/"><span class="tag">提示词</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%99%E7%A8%8B/"><span class="tag">教程</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%B0%E6%98%A5/"><span class="tag">新春</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%B0%E9%97%BB/"><span class="tag">新闻</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%97%A9%E6%8A%A5/"><span class="tag">早报</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%98%A5%E6%99%9A/"><span class="tag">春晚</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%99%BA%E8%B0%B1/"><span class="tag">智谱</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/"><span class="tag">本地部署</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/"><span class="tag">机器人</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%AD%A6BOT/"><span class="tag">武BOT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B5%8B%E8%AF%95%E8%87%AA%E5%8A%A8%E5%8C%96/"><span class="tag">测试自动化</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90/"><span class="tag">深度分析</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6/"><span class="tag">深度研究</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%91%E6%8A%80/"><span class="tag">科技</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%91%E6%8A%80%E5%B1%95%E6%9C%9B/"><span class="tag">科技展望</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%91%E6%8A%80%E7%83%AD%E6%A6%9C/"><span class="tag">科技热榜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%91%E6%8A%80%E8%B6%8B%E5%8A%BF/"><span class="tag">科技趋势</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%A7BOT/"><span class="tag">秧BOT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A8%8B%E5%BA%8F%E7%90%86%E8%A7%A3/"><span class="tag">程序理解</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"><span class="tag">编程工具</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BE%8E%E8%82%A1/"><span class="tag">美股</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"><span class="tag">英语学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/"><span class="tag">视频生成</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%B0%E5%BF%86%E7%B3%BB%E7%BB%9F/"><span class="tag">记忆系统</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BB%E4%B9%A6/"><span class="tag">读书</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B0%B7%E6%AD%8C/"><span class="tag">谷歌</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E5%90%91%E5%88%87%E9%9D%A2/"><span class="tag">面向切面</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A3%9E%E4%B9%A6/"><span class="tag">飞书</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A9%AC%E6%96%AF%E5%85%8B/"><span class="tag">马斯克</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo.svg" alt="内森淼文" height="28"></a><p class="is-size-7"><span>&copy; 2026 Nathan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="My GitHub Index" href="https://github.com/hydraxman"><i class="fab fa-github"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>